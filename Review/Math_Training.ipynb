{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73d3764e-1513-4bf4-b136-74cc3441065f",
   "metadata": {},
   "source": [
    "# Intro/Recap to Linear Algebra and Probability Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3390a9e5-673a-4971-a69b-1e53a1c93968",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Complete the _Data\\_Analysis\\_Training.ipynb_ tutorial\n",
    "\n",
    "## Objectives:\n",
    "Learn or refresh the following subjects:\n",
    "- Linear Algebra\n",
    "  - Vectors and Matrices\n",
    "  - Sparse Matrices\n",
    "- Probability Theory\n",
    "  - Random Variables\n",
    "  - Probability Distributions/Mass Functions\n",
    "  - Expected Value, Variance, and Covariance\n",
    "\n",
    "I believe that one of the most useful skills a data scientist can have is \"Google-Fu\", the ability to use search engines or search through code documentation to find answers to problems. As such, I will not be giving a complete tutorial here so much as a brief introduction to these math concepts. I will provide several examples of useful tools and how to use them along with brief notes to clarify some of the more confusing points. I highly recommend doing your own reading for everything we discuss both to learn the material better and to build your familiarity with useful references for these topics. Samir and I are more than happy to help with any questions you may have during this process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a963b55-c832-4dc6-a8d1-aacbc09df628",
   "metadata": {},
   "source": [
    "## Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecbc06c-17f8-42ef-a0d6-05b2189c06ca",
   "metadata": {},
   "source": [
    "Linear algebra is the lifeblood of the sciences. It is one of the more beautiful branches of mathematics and one of the most important tools for smoothly translating abstract ideas into the real world. I _highly_ recommend watching the following series of videos to build your intuition for linear algebra. Grant Sanderson does a masterful job of taking otherwise difficult and abstract concepts and making them easy to understand.  \n",
    "https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab  \n",
    "I will also link relevant videos from this series in the individual sections below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e386c6-7d37-41c1-bcaf-cfd4e1756a4b",
   "metadata": {},
   "source": [
    "### Vectors and Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65544cd7-27f3-4ee5-8caa-192a9696c2f0",
   "metadata": {},
   "source": [
    "Vectors and matrices are the fundamental objects of linear algebra. For the purposes of data science, a vector can be thought of as an ordered list of numbers, and a matrix can be thought of as either an ordered array of numbers or as a special kind of function that acts on vectors. In the case where each element of a vector represents spatial coordinates, matrices can represent linear spatial transformations, or spatial transformations that keep lines parallel and evenly spaced. In any case, matrices can always be written as an ordered array of numbers.  \n",
    "Here are Grant Sanderson's 3blue1brown videos introducing vectors and matrices:  \n",
    "- https://youtu.be/fNk_zzaMoSs\n",
    "- https://youtu.be/kYB8IZa5AuE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6282b19-7816-413a-b5a2-08b0f0138373",
   "metadata": {},
   "source": [
    "#### NumPy Syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b1135e-e648-429b-b92e-b9341483a7ff",
   "metadata": {},
   "source": [
    "Following are examples of vectors, matrices, and vector arithmetic, and matrix-vector multiplication as implemented in NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12374556-9b2b-45e5-ba2d-964e813ce1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As always, we start with import statements\n",
    "import numpy as np\n",
    "\n",
    "# Let's produce an example vector\n",
    "v1 = np.array([8, 6, 7, 5, 3, 0, 9])\n",
    "print(\"This is a 7-dimensional, or length 7, vector:\")\n",
    "print(v1)\n",
    "print(\"Note that vectors are usually displayed vertically rather than horizontally. In NumPy, 1D vectors are printed horizontally.\")\n",
    "\n",
    "# Let's produce an example matrix\n",
    "M1 = np.random.randint(0,10,(5,7))\n",
    "print(\"\\nThis is a 5x7 matrix:\")\n",
    "print(M1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68821cc-b392-4750-bfd3-2ee20d0e8f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectors of the same dimensionality can be added together or subtracted from each other. This is the same as element-wise addition.\n",
    "v2 = np.random.randint(0,10,7)\n",
    "print(\"Vector 1:\")\n",
    "print(v1)\n",
    "print(\"\\nVector 2:\")\n",
    "print(v2)\n",
    "print(\"\\nVector 1 plus vector 2:\")\n",
    "print(v1+v2)                           # The vector addition and subtraction syntax in NumPy is the same as normal addition and subtraction\n",
    "print(\"\\nVector 1 minus vector 2:\")\n",
    "print(v1-v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86764584-b58b-460f-a3e4-d9072e4af5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplying a vector by a single number (a \"scalar\") multiplies each entry of the vector by the number, \"scaling\" the vector uniformly.\n",
    "print(\"Vector 1:\")\n",
    "print(v1)\n",
    "print(\"\\nVector 1 times 3:\")\n",
    "print(v1*3)             # The vector scalar multiplication and division syntax in NumPy is the same as with normal multiplication and division\n",
    "print(\"\\nVector 2:\")\n",
    "print(v2)\n",
    "print(\"\\nVector 2 divided by 10:\")\n",
    "print(v2/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e4135e-216e-40a6-87e5-78ae6df12ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A vector and a matrix can be multiplied together if the matrix has the same number of columns as the vector is long\n",
    "print(\"Vector 1:\")\n",
    "print(v1)\n",
    "print(\"\\nMatrix 1:\")\n",
    "print(M1)\n",
    "print(\"\\nMatrix 1 acting on vector 1:\")\n",
    "print(np.matmul(M1,v1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60d5e9a-0946-46fb-9432-b3ce9668410c",
   "metadata": {},
   "source": [
    "#### Mathematical Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a192d984-e230-44a6-90d2-3e11841e04da",
   "metadata": {},
   "source": [
    "Multiplying a vector and a matrix uses the following formula. For a more intuitive interpretation of how and why this formula works, see the 3blue1brown videos by Grant Sanderson in the link at the top of the \"Vectors and Matrices\" section.  \n",
    "Let $A$ be an $m\\times n$ matrix defined by:\n",
    "$$\\begin{bmatrix} a_{11} & a_{12} & ... & a_{1n} \\\\ a_{21} & a_{22} & ... & a_{2n} \\\\ ... & ... & ... & ... \\\\ a_{m1} & a_{m2} & ... & a_{mn} \\end{bmatrix}$$  \n",
    "Let $b$ be a length-$n$ vector defined by:  \n",
    "$$\\begin{bmatrix} b_1 \\\\ b_2 \\\\ ... \\\\ b_n \\end{bmatrix}$$  \n",
    "The product $Ab$ is the length-$m$ vector:  \n",
    "$$\\begin{bmatrix} a_{11}b_1 + a_{12}b_2 + ... + a_{1n}b_n \\\\ a_{21}b_1 + a_{22}b_2 + ... + a_{2n}b_n \\\\ ... \\\\ a_{m1}b_1 + a_{m2}b_2 + ... + a_{mn}b_n \\end{bmatrix}$$  \n",
    "This process can be done by hand as follows:\n",
    "1. Check to make sure that the number of columns of the matrix is the same as the number of rows/entries of the vector. If they are not the same, you _cannot_ perform the operation. The result would be undefined.\n",
    "2. For the first row of the matrix, multiply the first column's entry by the first element of the vector, multiply the second column's entry by the second element of the vector, and so on.\n",
    "3. Add togther each of the products from step 2. This sum is the first entry of the result vector.\n",
    "4. Repeat steps 2 and 3 for each row of the matrix to get each of the remaining entries for the result vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cc71f4-f4dc-4d6d-9551-6893f1e5596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example of matrix-vector multiplication\n",
    "M2 = np.array([[3,4],     # The newline and indentation here aren't necessary, but they make the matrix easier to read.\n",
    "               [1,2]])\n",
    "v3 = np.array([1,2])\n",
    "print(\"Vector:\")\n",
    "print(v3)\n",
    "print(\"\\nMatrix:\")\n",
    "print(M2)\n",
    "print(\"\\nMatrix acting on vector:\")\n",
    "print(np.matmul(M2,v3))\n",
    "print(\"\\nThis follows because (3*1)+(4*2)=11 and (1*1)+(2*2)=5.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369df58c-b371-49b6-a88c-a2dc9aeae644",
   "metadata": {},
   "source": [
    "#### Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a74b0a-727f-4707-82cc-7b7f8aadf832",
   "metadata": {},
   "source": [
    "We can multiply two matrices by each other to get another matrix as a result. Note that the result is different depending on the order in which the matrices are multiplied. The matrix on the _left_ must have the same number of columns as the matrix on the _right_ has rows. The resulting matrix has the same number of rows as the matrix on the _left_ and the same number of columns as the matrix on the _right_. In general, (which, in mathematician speak, means \"literally always\") two matrices of size $m\\times n$ and size $p\\times q$ can be multiplied together if $n=p$, and the resulting matrix has size $m\\times q$. This is actually identical behavior to matrix-vector multiplication if we consider a vector to be a matrix with only one column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a5e2cc-d9aa-4c37-ba5d-31900b7f58e6",
   "metadata": {},
   "source": [
    "The formula for matrix-matrix multiplication is essentially identical to matrix-vector multiplication if we consider each column of the matrix on the right to be an individual vector.\n",
    "Let $A$ be an $m\\times n$ matrix defined by:\n",
    "$$\\begin{bmatrix} a_{11} & a_{12} & ... & a_{1n} \\\\ a_{21} & a_{22} & ... & a_{2n} \\\\ ... & ... & ... & ... \\\\ a_{m1} & a_{m2} & ... & a_{mn} \\end{bmatrix}$$  \n",
    "Let $B$ be an $n\\times r$ matrix defined by:\n",
    "$$\\begin{bmatrix} b_{11} & b_{12} & ... & b_{1r} \\\\ b_{21} & b_{22} & ... & b_{2r} \\\\ ... & ... & ... & ... \\\\ b_{n1} & b_{n2} & ... & b_{nr} \\end{bmatrix}$$  \n",
    "The product $AB$ is the $m\\times r$ matrix:  \n",
    "$$\\begin{bmatrix} a_{11}b_{11} + a_{12}b_{21} + ... + a_{1n}b_{n1} & a_{11}b_{12} + a_{12}b_{22} + ... + a_{1n}b_{n2} & ... & a_{11}b_{1r} + a_{12}b_{2r} + ... + a_{1n}b_{nr} \\\\ a_{21}b_{11} + a_{22}b_{21} + ... + a_{2n}b_{n1} & a_{21}b_{12} + a_{22}b_{22} + ... + a_{2n}b_{n2} & ... & a_{21}b_{1r} + a_{22}b_{2r} + ... + a_{2n}b_{nr} \\\\ ... & ... & ... & ...\\\\ a_{m1}b_{11} + a_{m2}b_{21} + ... + a_{mn}b_{n1} & a_{m1}b_{12} + a_{m2}b_{22} + ... + a_{mn}b_{n2} & ... & a_{m1}b_{1r} + a_{m2}b_{2r} + ... + a_{mn}b_{nr} \\end{bmatrix}$$  \n",
    "For a more intuitive understanding of matrix multiplication, check out this 3blue1brown video: https://youtu.be/XkY2DOUCWMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c1dc2d-232a-4d36-9aad-8e6fc8b57163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of matrix-matrix multiplication\n",
    "M3 = np.array([[3,2], \n",
    "               [1,4]])\n",
    "M4 = np.array([[1,2], \n",
    "               [4,3]])\n",
    "print(\"Left-hand matrix:\")\n",
    "print(M3)\n",
    "print(\"\\nRight-hand matrix:\")\n",
    "print(M4)\n",
    "print(\"\\nResult of matrix multiplication:\")\n",
    "print(np.matmul(M3,M4))\n",
    "print(\"\\nThis follows because (3*1)+(2*4)=11, (3*2)+(2*3)=12, (1*1)+(4*4)=17, and (1*2)+(4*3)=14.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5544cae4-d28f-4094-9045-e63e93d27ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same example of matrix-matrix multiplication as above, but with the order of the matrices reversed\n",
    "print(\"Left-hand matrix:\")\n",
    "print(M4)\n",
    "print(\"\\nRight-hand matrix:\")\n",
    "print(M3)\n",
    "print(\"\\nResult of matrix multiplication:\")\n",
    "print(np.matmul(M4,M3))\n",
    "print(\"\\nThis follows because (1*3)+(2*1)=5, (1*2)+(2*4)=10, (4*3)+(3*1)=15, and (4*2)+(3*4)=20.\\nNote that this is not the same as before.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4196e471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a special kind of matrix called a 2D rotation matrix.\n",
    "angle_deg = 90\n",
    "angle_rad = np.radians(angle_deg)\n",
    "rotation_mat = np.array([[np.cos(angle_rad), -np.sin(angle_rad)], \n",
    "                         [np.sin(angle_rad), np.cos(angle_rad)]])\n",
    "print(\"This is a rotation matrix of\", angle_deg, \"degrees:\")\n",
    "print(rotation_mat)\n",
    "print(\"\\nThe cleaned version:\")\n",
    "print(np.round(rotation_mat,3))\n",
    "print(\"\\nIf a 2D vector is a point in space, multiplication by this matrix rotates the point abount the origin by\", angle_deg, \"degrees\")\n",
    "\n",
    "# Lets import a graphing library to illustrate this\n",
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "# Let's plot an initial vector in green\n",
    "v_init = np.random.random(2)\n",
    "origin = np.array([0.0, 0.0])\n",
    "plt.plot([origin[0], v_init[0]], [origin[1], v_init[1]], \"g\", label=\"Initial\")\n",
    "\n",
    "# Let's plot the rotated vector in red\n",
    "v_rot = np.matmul(rotation_mat, v_init)\n",
    "plt.plot([origin[0], v_rot[0]], [origin[1], v_rot[1]], \"r\", label=\"Rotated\")\n",
    "\n",
    "# Now we plot the vectors, making sure to label the graph and use an appropriate viewing window\n",
    "plt.title(\"A Vector Rotated by {} Degrees\".format(angle_deg))\n",
    "plt.xlim(-np.sqrt(2), np.sqrt(2))\n",
    "plt.ylim(-np.sqrt(2), np.sqrt(2))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6590908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do this with a different angle\n",
    "angle_deg = np.round(np.random.random()*360, 3)\n",
    "angle_rad = np.radians(angle_deg)\n",
    "rotation_mat = np.array([[np.cos(angle_rad), -np.sin(angle_rad)], \n",
    "                         [np.sin(angle_rad), np.cos(angle_rad)]])\n",
    "print(\"This is a rotation matrix of\", angle_deg, \"degrees:\")\n",
    "print(rotation_mat)\n",
    "print(\"\\nThe cleaned version:\")\n",
    "print(np.round(rotation_mat,3))\n",
    "\n",
    "# Set the size of the plot\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "# Let's plot an initial vector in green\n",
    "v_init = np.random.random(2)\n",
    "origin = np.array([0.0, 0.0])\n",
    "plt.plot([origin[0], v_init[0]], [origin[1], v_init[1]], \"g\", label=\"Initial\")\n",
    "\n",
    "# Let's plot the rotated vector in red\n",
    "v_rot = np.matmul(rotation_mat, v_init)\n",
    "plt.plot([origin[0], v_rot[0]], [origin[1], v_rot[1]], \"r\", label=\"Rotated\")\n",
    "\n",
    "# Now we plot the vectors, making sure to label the graph and use an appropriate viewing window\n",
    "plt.title(\"A Vector Rotated by {} Degrees\".format(angle_deg))\n",
    "plt.xlim(-np.sqrt(2), np.sqrt(2))\n",
    "plt.ylim(-np.sqrt(2), np.sqrt(2))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d009f57-a401-44b6-bea3-cb44b75879a9",
   "metadata": {},
   "source": [
    "#### Matrix Inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5e92b5-d482-4a62-804d-0c5bd5532252",
   "metadata": {},
   "source": [
    "Almost every square matrix has an _inverse_. The inverse of a matrix is another matrix that undoes the effect of the first matrix when acting upon a vector. In more technical terms, the result of multiplying a matrix by its inverse, regardless of whether the inverse is on the right or on the left, is the _identity matrix_, which is a matrix of the appropriate size that consists entirely of zeros except for a line of ones along the diagonal. An identity matrix, regardless of its size, is written $I$. Multiplying the identity matrix by any appropriately sized matrix or vector simply returns that matrix or vector unchanged. In order for a square matrix to _not_ have an inverse, its columns must be _linearly dependent_, but that's beyond the scope of this tutorial and usually only applies to specially constructed matrices, not to the somewhat random matrices we usually deal with in data science. There are ways to construct a pseudo-inverse for non-square matrices, but that is also beyond the scope of this tutorial.  \n",
    "This is Grant Sanderson's 3blue1brown video about matrix inverses and more: https://youtu.be/uQhTuRlWMxw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8c0cc1-e41b-41fe-8469-9408938083ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of a matrix and its inverse\n",
    "M5 = np.random.randint(1,10,(3,3))\n",
    "print(\"Matrix to be inverted:\")\n",
    "print(M5)\n",
    "print(\"\\nInverse matrix:\")\n",
    "print(np.linalg.inv(M5))\n",
    "print(\"\\nResult of matrix multiplication:\")\n",
    "print(np.matmul(M5,np.linalg.inv(M5)))\n",
    "print(\"\\nCleaned up result:\")\n",
    "print(np.round(np.matmul(M5,np.linalg.inv(M5))))\n",
    "print(\"\\nResult of matrix multiplication with order reversed:\")\n",
    "print(np.matmul(np.linalg.inv(M5),M5))\n",
    "print(\"\\nCleaned up result:\")\n",
    "print(np.round(np.matmul(np.linalg.inv(M5),M5)))\n",
    "print(\"\\nDue to the use of floating-point arithmetic, numerically computed inverses will be close but not exactly equal to true inverses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d8fe7d",
   "metadata": {},
   "source": [
    "#### System of Linear Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1828596e",
   "metadata": {},
   "source": [
    "With matrix inverses in hand, we can use matrix equations to easily solve most systems of linear equations. Suppose we have the following system of equations:  \n",
    "$a_1x_1 + a_2x_2 + a_3x_3 = b_1$  \n",
    "$a_4x_1 + a_5x_2 + a_6x_3 = b_2$  \n",
    "$a_7x_1 + a_8x_2 + a_9x_3 = b_3$  \n",
    "We can use normal algebraic methods to find the values of $x_1$, $x_2$, and $x_3$ that satisfy all three equations. However, this can be tedious and time consuming, and could become completely untenable if the system of equations were expanded to include more unknowns or more equations. To figure out a faster way to solve these equations, note that we can rewrite the system of equations in the form of matrix-vector multiplication as follows:  \n",
    "Let the matrix $A$ be:  \n",
    "$$\\begin{bmatrix} a_1 & a_2 & a_3 \\\\ a_4 & a_5 & a_6 \\\\ a_7 & a_8 & a_9 \\end{bmatrix}$$  \n",
    "Let the vector $x$ be:  \n",
    "$$\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}$$  \n",
    "Let the vector $b$ be:  \n",
    "$$\\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix}$$  \n",
    "The equation $Ax = b$ is then equivalent to: \n",
    "$$\\begin{bmatrix} a_1x_1 + a_2x_2 + a_3x_3 \\\\ a_4x_1 + a_5x_2 + a_6x_3 \\\\ a_7x_1 + a_8x_2 + a_9x_3 \\end{bmatrix} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix}$$  \n",
    "which is equivalent to our above system of equations. If we multiply both sides of the matrix equation by the inverse of $A$, denoted as $A^{-1}$, then we get that $A^{-1}Ax = x = A^{-1}b$. By simply multiplying $b$ by $A^{-1}$ (or the non-square equivalent of an inverse if necessary), we can immediately solve for each of $x_1$, $x_2$, and $x_3$ using a linear algebra computer program.  \n",
    "Here's a video tutorial that shows another way to use Python and linear algebra to solve systems of linear equations: https://youtu.be/JHQgm5On3D0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67e0f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using linear algebra to solve a system of linear equations.\n",
    "\n",
    "\"\"\"Equations:\n",
    "w + 3x + 2y + 5z = 18\n",
    "8w + 12x + 3y + 2z = 32\n",
    "8w + 6x + 7y + 5z = 3\n",
    "2w + 9x + 8y + 6z = 7\"\"\"\n",
    "\n",
    "A = np.array([[1, 3, 2, 5], \n",
    "              [8, 12, 3, 2], \n",
    "              [8, 6, 7, 5], \n",
    "              [2, 9, 8, 6]])\n",
    "b = np.array([18, 32, 3, 7])\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(\"\\nVector b:\")\n",
    "print(b)\n",
    "\n",
    "# Get the inverse of A\n",
    "A_inv = np.linalg.inv(A)\n",
    "print(\"\\nA inverse:\")\n",
    "print(A_inv)\n",
    "\n",
    "# Get what 'x' must be\n",
    "x = np.matmul(A_inv, b)\n",
    "print(\"\\nSolution to system of equations:\")\n",
    "print(\"w={}, \\nx={}, \\ny={}, \\nz={}\".format(x[0], x[1], x[2], x[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf0c2c8",
   "metadata": {},
   "source": [
    "#### Transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60635a47",
   "metadata": {},
   "source": [
    "We sometimes need to turn a matrix or vector on its side. For example, we may need to multiply a vector on the left of a matrix instead of on the right for certain algebraic operations. If we have an $n\\times n$ square matrix $A$ and a length-$n$ vector $b$, then the multiplication $Ab$ is allowed, but the mutliplication $bA$ is not allowed because it would be as if an $n\\times 1$ matrix and an $n\\times n$ matrix were being multiplied together, which would result in a dimension mismatch. Instead, we take the _transpose_ of $b$, denoted by $b^T$. This results in a $1\\times n$ matrix (or _left-hand vector_), which no longer causes a dimension mismatch. In general, for any $m\\times n$ matrix  \n",
    "$$A = \\begin{bmatrix} a_{11} & a_{12} & ... & a_{1n} \\\\ a_{21} & a_{22} & ... & a_{2n} \\\\ ... & ... & ... & ... \\\\ a_{m1} & a_{m2} & ... & a_{mn} \\end{bmatrix}$$  \n",
    "we have that  \n",
    "$$A^T = \\begin{bmatrix} a_{11} & a_{21} & ... & a_{m1} \\\\ a_{12} & a_{22} & ... & a_{m2} \\\\ ... & ... & ... & ... \\\\ a_{1n} & a_{2n} & ... & a_{mn} \\end{bmatrix}$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc764091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create an arbitrary matrix to transpose\n",
    "M6 = np.random.randint(0,10,(5,5))\n",
    "print(\"The original matrix:\")\n",
    "print(M6)\n",
    "\n",
    "# Let's show the transpose of this matrix\n",
    "M6T = M6.T\n",
    "print(\"\\nThe transposed matrix:\")\n",
    "print(M6T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d59415",
   "metadata": {},
   "source": [
    "#### Norms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd7b945",
   "metadata": {},
   "source": [
    "The _norm_ of a vector is a generalized way to talk about a vector's size or length in space. There are many different ways to define a norm, the most common of which is to define a norm with Euclidean distance, which gives the Euclidean distance from the origin to the tip of the vector (assuming that the components of the vector represent positions in space). In general, for a vector $v$ with  \n",
    "$$v=\\begin{bmatrix} v_1 \\\\ v_2 \\\\ ... \\\\ v_n \\end{bmatrix}$$  \n",
    "the Euclidean norm of $v$ is  \n",
    "$$||v||=\\sqrt{v_1^2 + v_2^2 + ... + v_n^2}$$  \n",
    "For more information about different vector norms (which will come in handy later in the course), see this video: https://youtu.be/5fN2J8wYnfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62960db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a few vectors and their norms\n",
    "v4 = np.random.randint(-10,10,5)\n",
    "print(\"Our test vector is:\")\n",
    "print(v4)\n",
    "print(\"Its norm is\", np.linalg.norm(v4))\n",
    "\n",
    "# Multiplying all of the entries of a vector by a scalar (non-vector) value also scales the norm\n",
    "print(\"\\nOur test vector scaled by 10:\")\n",
    "print(v4*10)\n",
    "print(\"The new norm for this scaled vector is {}, which should by 10 times the previous norm.\".format(np.linalg.norm(v4*10)))\n",
    "\n",
    "# Norms scale with the absolute value of the vector's scaling factor\n",
    "print(\"\\nOur test vector scaled by -0.1:\")\n",
    "print(v4*-0.1)\n",
    "print(\"The new norm for this scaled vector is {}, which should by 0.1 times the previous norm.\".format(np.linalg.norm(v4*-0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2798bb24",
   "metadata": {},
   "source": [
    "#### Eigenvectors and Eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1f2a7e",
   "metadata": {},
   "source": [
    "In short, an _eigenvector_ of a matrix is a vector such that multiplying the matrix by the vector yields the same vector multiplied by a constant. That constant is known as an _eigenvalue_. In general, for a matrix $A$ with eigenvectors $v_1$ to $v_n$ and eigenvalues $\\lambda_1$ to $\\lambda_n$, we have:  \n",
    "$Av_i = \\lambda_iv_i$  \n",
    "for each $i$ from 1 to n.  \n",
    "Each square matrix has as many eigenvalues and eigenvectors as it has rows, but these eigenvalues and eigenvectors may be complex/imaginary or may be _degenerate_, or identical copies of each other.  \n",
    "This is the 3blue1brown video about eigenvectors and eigenvalues: https://youtu.be/PFDu9oVAE-g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c1bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create an arbitrary matrix to generate eigenvectors and eigenvalues\n",
    "M7 = np.random.randint(0,10,(5,5))\n",
    "print(\"The original matrix:\")\n",
    "print(M7)\n",
    "\n",
    "# Let's get its eigenvectors and eigenvalues\n",
    "eig_vals, eig_vecs = np.linalg.eig(M7)\n",
    "print(\"\\nThe eigenvalues, which might be complex-valued:\")\n",
    "print(eig_vals)\n",
    "print(\"\\nThe eigenvectors, which may be complex-valued\")\n",
    "for i in range(len(eig_vals)):\n",
    "    print(eig_vecs[:,i], end=\"\\n\\n\")\n",
    "\n",
    "# Note that multiplying the matrix by any of its eigenvectors is the same as scaling the eigenvector by the eigenvalue\n",
    "for i in range(len(eig_vals)):\n",
    "    print(\"\\nThe original matrix times eigenvector number {}:\".format(i+1))\n",
    "    print(np.matmul(M7,eig_vecs[:,i]))\n",
    "    print(\"\\nEigenvector number {} scaled by its eigenvalue:\".format(i+1))\n",
    "    print(eig_vecs[:,i]*eig_vals[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becb901c",
   "metadata": {},
   "source": [
    "#### Determinants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af72b06b",
   "metadata": {},
   "source": [
    "The _determinant_ of a matrix is a value calculated from each entry of the matrix that encodes certain properties of the matrix. For example, if the matrix represents a spatial transformation, then the determinant represents the factor by which the area/volume/n-dimensional equivalent of a shape is scaled by the matrix transformation. The determinant of a non-invertible matrix is always 0.  \n",
    "This is Grant Sanderson's video about determinants: https://youtu.be/Ip3X9LOh2dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2684e602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A rotation matrix doesn't scale vectors through space, so it should have a determinant of 1\n",
    "print(\"A\", angle_deg, \"degree rotation matrix:\")\n",
    "print(rotation_mat)\n",
    "print(\"\\nIts determinant is {}, which should be extremely close to 1.\".format(np.linalg.det(rotation_mat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9dc09c-bd16-40ad-b549-57d1ab4ed837",
   "metadata": {},
   "source": [
    "### Sparse Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0927470",
   "metadata": {},
   "source": [
    "Sparse matrices are matrices in which a large number of entries are equal to zero or so close to zero that they might as well be exactly equal to zero for computational purposes. Most Python packages that deal with matrices have special methods for handling sparse matrices in memory-efficient ways. This is critical for many datasets that are extremely large but largely sparse. There are several ways to store sparse matrices efficiently, each with different pros and cons depending on their intended usage. Some data structures allow for much more efficient modifications to existing sparse matrices but result in slower arithmetic operations, while others allow for extremely quick arithmetic operations at the expense of very slow modifications to existing matrices. You can learn more about these pros and cons for your own projects at https://docs.scipy.org/doc/scipy/reference/sparse.html by clicking through the \"Sparse array classes\" pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7472c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "from scipy import sparse\n",
    "\n",
    "# Let's make our initial sparse matrix\n",
    "sparse_matrix = np.zeros((10,10))\n",
    "num_non_zero = 10\n",
    "non_zero_rows = np.random.randint(0,10,num_non_zero)\n",
    "non_zero_cols = np.random.randint(0,10,num_non_zero)\n",
    "for i in range(num_non_zero):\n",
    "    sparse_matrix[non_zero_rows[i],non_zero_cols[i]] = np.random.randint(-9,10)\n",
    "print(\"Our sparse matrix:\")\n",
    "print(sparse_matrix)\n",
    "\n",
    "# We can convert this dense-format matrix into a sparse-format matrix to be more efficient\n",
    "sparse_matrix_csc = sparse.csc_matrix(sparse_matrix)    # I chose the CSC format arbitrarily\n",
    "print(\"\\nOur sparse matrix stored in CSC format:\")\n",
    "print(sparse_matrix_csc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f87de63-945a-4f73-aa2f-e4d2849a9613",
   "metadata": {},
   "source": [
    "## Probability Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ed43eb",
   "metadata": {},
   "source": [
    "Probability theory relates to the representation and quantification of uncertainty. All real-world data contains some degree of random noise, and many real-world systems are best modeled as stochastic processes where outcomes must be assigned probability values. As such, probability and theory is essential for data science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2ada99-17a4-4489-86a4-9b96ea864385",
   "metadata": {},
   "source": [
    "### Random Variables and Probability Distributions/Mass Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94745860",
   "metadata": {},
   "source": [
    "A _random variable_ is a variable that can take one of various values (sometimes called _events_ or _states_). Random variables can be discrete (finitely many possible values) or continuous (infinitely many possible values). Random variables can even be vector valued.  \n",
    "A _probability distribution_ is a description of how likely each state of a random variable is to occur. _Probability density functions_ describe continuous random variables, while _probability mass functions_ decribe discrete random variables. These probability functions map random variable states to probability values.  \n",
    "Strictly speaking, probability density functions and probability mass functions are fundamentally different functions. Mass functions can be operated upon with basic arithmetic operations, while density functions require calculus operations. However, by substituting summations with integrals and exact states with state ranges, we can describe density functions and mass functions in nearly identical ways.  \n",
    "For a state $x_1$ of a random variable <b>x</b>, we have that $P(x_1)$ is the probability that <b>x</b> is in the state $x_1$. In the discrete case, this can be written $P($<b>x</b> $=x_1)$, and the output of $P(x_i)$ must be a real number between 0 and 1, inclusive, for any and all states $x_i$. If <b>x</b> is a _continuous_ random variable, we have that $P(x_1\\leq$ <b>x</b>$\\leq x_2)=\\int_{x_1}^{x_2}P(x)dx$ is the probability that the state of <b>x</b> is in the range between $x_1$ and $x_2$. The density function $P(x)$ can give any nonnegative real value as an output for any state $x$ of <b>x</b>, but the integral $\\int_{x_1}^{x_2}P(x)dx$ must always give a value between 0 and 1, inclusive. Since the probabilities for each possibile value of a random value must sum to one, we must always have that $\\sum_{\\forall x_i}P(x_i)=1$ or $\\int_{-\\infty}^{\\infty}P(x)dx=1$.  \n",
    "As a toy example, we could have the discrete random variable <b>z</b> with probability distribution $P(-1)=0.2$, $P(0)=0.3$, and $P(1)=0.5$. In this example, we would expect to observe <b>z</b> $=-1$ one fifth of the time, <b>z</b> $=0$ three tenths of the time, and <b>z</b> $=1$ half of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e4c21d",
   "metadata": {},
   "source": [
    "#### Common Probability Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76634ac",
   "metadata": {},
   "source": [
    "- Uniform distribution (discrete):  \n",
    "  - For a space of $k$ possible values $x_i$, $P(x_i) = \\frac{1}{k}$ for valid values $x_i$, $P(x)=0$ otherwise.\n",
    "- Uniform distribution (continuous):  \n",
    "  - For a space of possible values $x$ between $a$ and $b$, $P(x) = \\frac{1}{b-a}$ for $a\\leq x\\leq b$, $P(x)=0$ otherwise.\n",
    "- Bernoulli distribution (discrete):\n",
    "  - For a binary random variable with possible values $0$ and $1$, given that $P(1)=p$ and $P(0)=1-p$, we have $P(x)=p^x(1-p)^{1-x}$\n",
    "- Normal distribution (continuous):\n",
    "  - For a space of possible values $x$ where $x$ can be any real number, $P(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}$, where $\\sigma$ is standard deviation and $\\mu$ is the mean value.\n",
    "- Empirical distribution (discrete):\n",
    "  - A similar distribution to the uniform distribution where the values $x_i$ are measured, as from a sample. For a set of $k$ measurements $x_i$, where the measured value of $x_i$ is repeated in $n_i$ of the measurements, we have $P(x_i) = \\frac{n_i}{k}$. For example, if we have the measurements 1, 3, 3, 5, then we would have $P(1)=\\frac{1}{4}$, $P(3)=\\frac{1}{2}$, $P(5)=\\frac{1}{4}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8574d001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import SciPy's stats module to illustrate probability distributions\n",
    "from scipy import stats\n",
    "\n",
    "# Let's get a normal distribution centered at 5 with a standard deviation of 2\n",
    "# More distributions can be found at https://docs.scipy.org/doc/scipy/reference/stats.html\n",
    "norm_dist = stats.norm(loc=5, scale=2)\n",
    "\n",
    "# Here's the syntax for drawing values from the distribution; the syntax is the same for other distributions in the scipy.stats module\n",
    "norm_vals = norm_dist.rvs(size=10)\n",
    "print(\"Here are 10 values drawn from a normal distribution with mean 5 and standard deviation 2:\")\n",
    "print(norm_vals)\n",
    "\n",
    "# We can visualize these values as points on a numberline\n",
    "plt.scatter(norm_vals, np.zeros(len(norm_vals)), label=\"Drawn Values\")\n",
    "plt.plot([norm_dist.mean(), norm_dist.mean()], [-0.01, 0.01], label=\"Distribution Mean\")\n",
    "plt.ylim(-0.05,0.05)\n",
    "plt.title(\"Normal Distribution Draws\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f1163f-d27e-48db-a408-9146d6f1052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can easily plot the probability density function of our normal distribution with the stats module\n",
    "xs = np.linspace(-5, 15, 1000)    # The values along which to plot the distribution\n",
    "ys = norm_dist.pdf(xs)           # The outputs of the function P(x)\n",
    "plt.plot(xs, ys, label=\"Probability Density Function\")\n",
    "plt.scatter(norm_vals, np.zeros(len(norm_vals)), label=\"Drawn Values\")\n",
    "plt.title(\"Normal Distribution\")\n",
    "plt.ylim(-0.01,0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca7c5d4",
   "metadata": {},
   "source": [
    "#### Cumulative Distribution Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b357bc",
   "metadata": {},
   "source": [
    "A \"cumulative distribution function\" is the integral from $-\\infty$ to some value $x$ of a continuous probability distribution $P(x)$. Expressed mathematically, we have $CDF(x)=\\int_{-\\infty}^xP(x)dx=P($<b>x</b>$<x)$. Many Python packages, like scipy.stats, have methods to quickly calculate CDFs of continuous probability distributions. Since $CDF(x)$ never decreases as $x$ increases, we can calculate $P(x_1<$<b>x</b>$<x_2)$ as $P(x_1<$<b>x</b>$<x_2)=CDF(x_2)-CDF(x_1)$. This is quite easy to calculate in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a012ce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use cumulative distribution functions to easily compute the probability of continuous random variables falling in a range\n",
    "stand_norm_dist = stats.norm(loc=0, scale=1)    # The same syntax works for any distribution in scipy.stats, e.g. stats.beta(a,b)\n",
    "x1 = -1                                          \n",
    "x2 = 1\n",
    "print(\"The probability of a draw from a standard normal distribution falling between {} and {}:\".format(x1,x2))\n",
    "print(stand_norm_dist.cdf(x2) - stand_norm_dist.cdf(x1))\n",
    "print(\"This is the probability of falling within one standard deviation of the mean.\")\n",
    "\n",
    "# Let's repeat this for a few other ranges\n",
    "for _ in range(5):\n",
    "    x1 = round(np.random.random()*6 - 3, 3)\n",
    "    x2 = round(np.random.random()*(3-x1) + x1, 3)\n",
    "    print(\"\\nThe probability of a draw from a standard normal distribution falling between {} and {}:\".format(x1,x2))\n",
    "    probability = stand_norm_dist.cdf(x2) - stand_norm_dist.cdf(x1)\n",
    "    print(probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210354a6-4a6a-4b3f-bae1-a1167e4e1e3a",
   "metadata": {},
   "source": [
    "### Statistical Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3acd779",
   "metadata": {},
   "source": [
    "#### Expected Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585af1ce",
   "metadata": {},
   "source": [
    "The expected value of a random variable is the mean of all its possible values weighted by the likelihood of each value occurring. It is the value we expect the mean to converge to as we draw infinitely many samples from the random variable. Expected value is calculated as follows: $\\mathbb{E}[$<b>x</b>$]=\\sum_xP(x)x$ for discrete cases or $\\mathbb{E}[$<b>x</b>$]=\\int_xP(x)xdx$ for continuous cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe45e4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1afdaf90",
   "metadata": {},
   "source": [
    "#### Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97122db5",
   "metadata": {},
   "source": [
    "Variance is a measure of deviation of a random variable from its expected value. It is the expected squared difference between a random variable's states and its expected value, which can be written as follows:  \n",
    "$Var($<b>x</b>$)=\\mathbb{E}[(x-\\mathbb{E}[$<b>x</b>$])^2]=\\sum_xP(x)(x-\\mathbb{E}[$<b>x</b>$])^2=\\sum_xP(x)\\left(x-\\sum_xP(x)x\\right)^2$  \n",
    "In practice, however, we always use a computer to calculate variance. The square root of the variance is known as _standard deviation_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3166f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0886252d",
   "metadata": {},
   "source": [
    "#### Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56bd13c",
   "metadata": {},
   "source": [
    "Covariance is a measure of how much two random variables vary linearly together. Covariance only applies when two random variables are distributed jointly: each draw from <b>x</b> is paired with a corresponding draw from <b>y</b>. Covariance is defined mathematically as follows:  \n",
    "$Cov($<b>x</b>$,$<b>y</b>$)=\\mathbb{E}[(x-\\mathbb{E}[$<b>x</b>$])(y-\\mathbb{E}[$<b>y</b>$])]$  \n",
    "For a sample of $N$ draws from <b>x</b> and <b>y</b>, or an empirical distribution, this gives us:  \n",
    "$Cov($<b>x</b>$,$<b>y</b>$)=\\frac{1}{N-1}\\sum_{i=1}^N(x_i-\\bar{x})(y_i-\\bar{y})$  \n",
    "If we know the true mean values of <b>x</b> and <b>y</b>, then we have:  \n",
    "$Cov($<b>x</b>$,$<b>y</b>$)=\\frac{1}{N}\\sum_{i=1}^N(x_i-\\mathbb{E}[$<b>x</b>$])(y_i-\\mathbb{E}[$<b>y</b>$])$  \n",
    "although, again, we almost always use a computer to calculate covariance.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed9c5e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e07ac735",
   "metadata": {},
   "source": [
    "#### Measures of Common Probability Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3043cdc9",
   "metadata": {},
   "source": [
    "- Uniform distribution (discrete):  \n",
    "  - $\\mathbb{E}[$<b>x</b>$]=Mean($<b>x</b>$)=\\frac{\\sum_{i=1}^kx_i}{k}$\n",
    "  - $Var($<b>x</b>$)=\\frac{1}{k}\\sum_{i=1}^k(x_i-\\mathbb{E}[$<b>x</b>$])^2$\n",
    "- Uniform distribution (continuous):  \n",
    "  - $\\mathbb{E}[$<b>x</b>$]=\\frac{1}{b-a}\\int_a^bxdx=\\frac{1}{b-a}\\frac{b^2-a^2}{2}=\\frac{(b-a)(b+a)}{2(b-a)}=\\frac{a+b}{2}$\n",
    "  - $Var($<b>x</b>$)=\\frac{(b-a)^2}{12}$\n",
    "- Bernoulli distribution (discrete):\n",
    "  - $\\mathbb{E}[$<b>x</b>$]=p$\n",
    "  - $Var($<b>x</b>$)=p(1-p)$\n",
    "- Normal distribution (continuous):\n",
    "  - $\\mathbb{E}[$<b>x</b>$]=\\mu$\n",
    "  - $Var($<b>x</b>$)=\\sigma^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d708800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4de600a-50b1-47da-854a-4b85ab163cf3",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce91180-b59e-4e43-b5d8-abe5fe888e6c",
   "metadata": {},
   "source": [
    "https://people.duke.edu/~ccc14/sta-663-2017/13A_LinearAlgebra1.html  \n",
    "https://people.duke.edu/~ccc14/sta-663-2017/13B_LinearAlgebra2.html  \n",
    "https://people.duke.edu/~ccc14/sta-663-2017/13E_SparseMatrices.html  \n",
    "https://www.deeplearningbook.org/contents/linear_algebra.html  \n",
    "https://www.deeplearningbook.org/contents/prob.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ce9486",
   "metadata": {},
   "source": [
    "Made by Adam Kotter, copyright 2022"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
