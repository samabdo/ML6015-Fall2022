{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c06c1ee5-700d-4169-9ea1-f5a0359d07b0",
   "metadata": {},
   "source": [
    "# Intro/Recap to Data Analysis in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387d46d5-bcf1-441c-9929-a9af07d5bf4c",
   "metadata": {},
   "source": [
    "## Objectives:\n",
    "Learn or refresh the following skills:\n",
    "- Data manipulation\n",
    "  - Statistics packages\n",
    "  - File management\n",
    "  - Pandas\n",
    "- Data visualization\n",
    "- Regular expressions  \n",
    "\n",
    "I believe that one of the most useful skills a data scientist can have is \"Google-Fu\", the ability to use search engines or search through code documentation to find answers to problems. As such, I will not be giving a complete tutorial here so much as a brief introduction to data analysis tools in Python. I will provide several examples of useful tools and how to use them along with brief notes to clarify some of the more confusing points. I highly recommend searching the documentation for all of the tools we discuss on your own to build your familiarity with the packages as well as to hone your \"Google-Fu\". Samir and I are more than happy to help with any questions you may have during this process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff0c046-b73f-46d7-b7a3-d8ec0709df1e",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadf4142-f947-45c2-b5ec-898821c23155",
   "metadata": {},
   "source": [
    "### Statistics Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ccd758-8ff2-4278-8de9-d172a351c3e7",
   "metadata": {},
   "source": [
    "We don't have to reinvent the wheel with every project. Using standard packages to do data analysis is usually a much better option then writing data analysis code from scratch. Not only are they more tested, and thus more reliable, code, they're also usually faster to execute than code written from scratch because they use faster languages like C on the backend.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc6994a-01a5-43b9-b64b-5a955482739a",
   "metadata": {},
   "source": [
    "#### NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c123ab65-33a3-4696-8390-b0798a8658b0",
   "metadata": {},
   "source": [
    "NumPy is likely the single most-used package in the Python community. It is the basis upon which almost every other Python module is based. NumPy is based around the idea of arrays and array operations. A NumPy array is like a more structured, possibly multi-dimensional version of a list. While not quite as flexible as a list, an array allows for more powerful numerical operations than lists.  \n",
    "For more tutorials on how to use NumPy, visit this link: https://numpy.org/learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e03ee2-7cfc-41f9-8114-c2cb6aea568f",
   "metadata": {},
   "source": [
    "##### Arrays and Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17812117-f8e7-47a1-b827-9d0dd82c74f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with import statements\n",
    "import numpy as np    # np is the standard alias for numpy\n",
    "import math           # the math module is for basic mathematical operations in Python but is usually made obsolete by NumPy\n",
    "\n",
    "# Taking the arithmetic mean of a list of numbers\n",
    "number_list = [8, 6, 7, 5, 3, 0, 9]\n",
    "list_avg = sum(number_list)/len(number_list)\n",
    "print(\"Average for list:\\t\\t\\t\\t\", list_avg)\n",
    "\n",
    "# Taking the arithmetic mean of an array of numbers\n",
    "number_array = np.array([8, 6, 7, 5, 3, 0, 9])\n",
    "array_avg = number_array.mean()\n",
    "print(\"Average for NumPy:\\t\\t\\t\\t\", array_avg)\n",
    "\n",
    "# Alternate way to use NumPy to take the arithmetic mean of a list of numbers\n",
    "np_list_mean = np.mean(number_list)\n",
    "print(\"Average for list using NumPy:\\t\\t\\t\", np_list_mean)\n",
    "\n",
    "# Taking the standard deviation of a list of numbers\n",
    "list_stdev = math.sqrt(sum([(value - list_avg)**2 for value in number_list])/len(number_list))\n",
    "print(\"Standard deviation for list:\\t\\t\\t\", list_stdev)\n",
    "\n",
    "# Taking the standard deviation of an array of numbers\n",
    "array_stdev = number_array.std()\n",
    "print(\"Standard deviation for NumPy:\\t\\t\\t\", array_stdev)\n",
    "\n",
    "# Alternate way to use NumPy to take the standard deviation of a list of numbers\n",
    "np_list_stdev = np.std(number_list)\n",
    "print(\"Standard deviation for NumPy using list:\\t\", np_list_stdev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137fbb57-9a03-4378-9cd8-8b945f6f6767",
   "metadata": {},
   "source": [
    "##### Multi-dimensional Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a49e706-64bf-4865-8b79-fcd37d43ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-dimensional arrays\n",
    "array_2d = np.array([[1,2,3], [4,5,6], [7,8,9]])    # a 3x3 array of integers\n",
    "array_3d = np.array([[[1,2,3],[4,5,6],[7,8,9]],     # a 3x3x3 array of integers\n",
    "                     [[10,11,12],[13,14,15],[16,17,18]], \n",
    "                     [[19,20,21],[22,23,24],[25,26,27]]])\n",
    "print(array_2d)\n",
    "print(\"\\n\")\n",
    "print(array_3d)\n",
    "\n",
    "# We can perform operations along different axes\n",
    "mean_2d_rows = np.mean(array_2d, axis=1)            # mean along 2D rows\n",
    "print(\"\\nMean of 2D rows:\\n\", mean_2d_rows)\n",
    "mean_2d_cols = np.mean(array_2d, axis=0)            # mean along 2D columns\n",
    "print(\"Mean of 2D columns:\\n\", mean_2d_cols)\n",
    "mean_3d_rows = np.mean(array_3d, axis=2)            # mean along 3D rows\n",
    "print(\"\\nMean of 3D rows:\\n\", mean_3d_rows)\n",
    "mean_3d_cols = np.mean(array_3d, axis=1)            # mean along 3D columns\n",
    "print(\"Mean of 3D columns:\\n\", mean_3d_cols)\n",
    "mean_3d_other = np.mean(array_3d, axis=0)           # mean along other 3D axis\n",
    "print(\"Mean of other 3D axis:\\n\", mean_3d_other)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b30068a-2af3-4f89-bd0e-ab2b4aee46ec",
   "metadata": {},
   "source": [
    "##### Array Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7582ea79-3388-42e8-9c90-01842520c30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can select slices of arrays to operate on only part of them\n",
    "array_2d = np.array([[1,2,3,4], [4,5,6,7], [7,8,9,10], [10,11,12,13]])    # a 4x4 array of integers\n",
    "array_3d = np.array([[[1,2,3,4],[4,5,6,7],[7,8,9,10],[10,11,12,13]],      # a 4x4x4 array of integers\n",
    "                     [[10,11,12,13],[13,14,15,16],[16,17,18,19],[19,20,21,22]], \n",
    "                     [[19,20,21,22],[22,23,24,25],[25,26,27,28],[27,28,29,30]]])\n",
    "print(\"Original 2D array:\\n\", array_2d)\n",
    "\n",
    "# Use a colon to mean \"everything up to\", \"everything past\", \"everything between\", or just \"everything\"\n",
    "slice_2d = array_2d[:2,1]                        # Every row up to, but not including, row 2 (the 3rd row); and just column 1 (the 2nd column)\n",
    "print(\"\\nRows up to 2, column 1:\\n\", slice_2d)   # Note that this will be in a 1D format, not a 2D format\n",
    "slice_2d = slice_2d.reshape(-1,1)                # Reshape the previous slice into a 2D format\n",
    "print(\"Rows up to 2, column 1:\\n\", slice_2d)\n",
    "slice_2d = array_2d[1:3,1:]                      # Rows between 1 (inclusive) and 3 (exclusive); and all columns starting with 1\n",
    "print(\"Rows between 1 and 3, columns 1 and up:\\n\", slice_2d)\n",
    "slice_2d = array_2d[:,-2:]                      # All rows; and the last 2 columns (everything past -2)\n",
    "print(\"All rows, last two columns:\\n\", slice_2d)\n",
    "\n",
    "# 3D slice example\n",
    "print(\"\\nOriginal 3D array:\\n\", array_3d)\n",
    "slice_3d = array_3d[:2,1:,-3:]\n",
    "print(\"\\nThe first two of the other axis, all rows from 1 and up, the last 3 columns:\\n\", slice_3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd6d9da-120b-41fb-aede-abd71a6eaaef",
   "metadata": {},
   "source": [
    "#### SciPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bea528e-dafe-486d-9a5c-ac6c4f138fd6",
   "metadata": {},
   "source": [
    "SciPy is a NumPy-based library for scientific and technical computing. SciPy contains far more tools than most data scientists will ever use, so we will only cover the most relevant ones here.  \n",
    "For more documentation on using SciPy, check out this page: https://docs.scipy.org/doc/scipy/tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51241765-c8e9-4289-b407-2338c1ef63be",
   "metadata": {},
   "source": [
    "##### Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f849d4c5-0901-4104-8e7b-095f2f3d3545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As always, we start with an import statement\n",
    "import scipy.stats as stats         # If you only need one tool from the stats module, it's usually more efficient to only import that tool\n",
    "\n",
    "# Linear regression                            # Docs: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html\n",
    "X = np.arange(4,25)                            # Get the x-coordinates between 4 and 24 (chosen arbitrarily)\n",
    "Y = 4*X + 2 + np.random.normal(size=len(X))    # Calculate the y-coordinates for a noisy line following y = 4x + 2\n",
    "results = stats.linregress(X, Y)               # Perform the linear regression\n",
    "print(\"Intercept is {}. This should be close to 2.\".format(results.intercept))\n",
    "print(\"Slope is {}. This should be close to 4.\".format(results.slope))\n",
    "\n",
    "# Descriptive statistics of an array, good for quickly assessing a large dataset\n",
    "print(\"\\nDescriptive statistics of an array:\")\n",
    "print(stats.describe(Y))\n",
    "print(\"The mean calculated with NumPy is {}, which matches the SciPy value of {}.\".format(np.mean(Y), stats.describe(Y).mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a64ba1-7637-49e3-b33d-7265698b9579",
   "metadata": {},
   "source": [
    "##### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae673d0d-246f-43a4-9869-650b99dcabf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the import statement for a SciPy optimization tool to find the minimum of a function\n",
    "from scipy.optimize import minimize                        # https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n",
    "\n",
    "# Create an arbitrary, 3D lambda function (essentially a quickly defined Python function)\n",
    "f = lambda x: x[0]**2 + np.abs(np.exp(x[1])-1) - np.cos(x[2])   # Infinitely many local minima with the same x0- and x1-coordinates\n",
    "\n",
    "# Find a minimum of the function\n",
    "x0 = np.random.random(3)\n",
    "result = minimize(f, x0)\n",
    "print(\"The found minimum of this function is {}, which should be close to [0,0,0].\".format(result.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f423b67-42bc-47c5-9aad-267784a56ff2",
   "metadata": {},
   "source": [
    "### File Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2637ae00-b419-4894-b875-3d2437b42bff",
   "metadata": {},
   "source": [
    "Most of the data you will see in medical informatics comes from files you download. More often than not, these files are formatted as CSV files with rows and columns. CSV files are usually opened on a desktop with a program like Excel. However, data can also come in TXT format, which is raw text. How do we use these files in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec7d8c6-ee66-468e-9fe9-6e10c6197de4",
   "metadata": {},
   "source": [
    "#### TXT files\n",
    "While there are other ways to do this, the following methods are the safest and, in my opinion, easiest ways to get data from a TXT file.  \n",
    "For more instructions on writing to files in addition to reading from files, check out this tutorial: https://www.geeksforgeeks.org/file-handling-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631378e-b18d-4cd0-bdee-77d1ce037f85",
   "metadata": {},
   "source": [
    "##### With Open Readlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb1a77-e0f0-4609-9eb8-7e206cb768c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method reads in each line in the file into a list of strings, leaving any '\\n' newline characters at the end of each string\n",
    "with open(\"hr_features.txt\", \"r\") as file:    # the 'file' object is only accessible within the 'with open' block\n",
    "    file_text = file.readlines()              # the 'file_text' object is accessible outside of the 'with open' block\n",
    "print(file_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d10a39d-3e7d-441a-8106-48e1347e7367",
   "metadata": {},
   "source": [
    "##### With Open Read Splitlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf1787b-34f8-42c7-9196-e646276d542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method reads in each line in the file into a list of strings, removing any '\\n' newline characters at the end of each string\n",
    "with open(\"hr_features.txt\", \"r\") as file:    # the \"r\" argument means that the file can only be read; we cannot edit the file\n",
    "    file_text = file.read().splitlines()\n",
    "print(file_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff2bc40-09d0-4113-b3cc-a35b0d0ec233",
   "metadata": {},
   "source": [
    "#### CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d49b25-fa05-4937-8fff-f5b03e2e7e80",
   "metadata": {},
   "source": [
    "You'll usually want to use Pandas to import a CSV file directly into a DataFrame, but it's useful to know how to read CSV files that aren't intended for use in a DataFrame.  \n",
    "A much more detailed tutorial on using the csv package in Python can be found here: https://www.geeksforgeeks.org/working-csv-files-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef249e4-b1f6-4250-99cb-c1f35734d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As always, we start with an import statement\n",
    "import csv\n",
    "\n",
    "# Because the file will only exist within the 'with open' block, we initialize a list to hold the data extracted in the 'with open' block\n",
    "data = []    # Note that a data structure other than a list may be ideal for different applications\n",
    "\n",
    "# We read in the file with a csv.reader object\n",
    "with open(\"iris.csv\", \"r\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        data.append(row)\n",
    "\n",
    "# Display a sample of the collected data\n",
    "for i in range(len(data)//10):\n",
    "    print(data[i])\n",
    "print(\"Note that this data is all formatted as strings. You have to convert data gathered this way from CSVs to numeric form separately.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc65842f-5788-453c-b4c9-1b98f838dddf",
   "metadata": {},
   "source": [
    "### Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6eb8fb-5a02-42dc-8f39-957042cf69cd",
   "metadata": {},
   "source": [
    "In addition to being adorable bears, Pandas is also an incredibly useful tool for data analysis. Pandas is a Python package that revolves around the idea of DataFrames. A DataFrame is an object similar to a NumPy array but with much greater functionality and flexibility. Let's look at the basics:  \n",
    "For more, see a largely comprehensive guide to the basics here: https://pandas.pydata.org/docs/user_guide/10min.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41f2c46-ed2b-46cd-b399-75e0b41a2cb4",
   "metadata": {},
   "source": [
    "#### DataFrame Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06e9ea7-c7b6-44ed-b6b9-93d951e8b3ca",
   "metadata": {},
   "source": [
    "DataFrames are commonly created from CSV files. Each DataFrame is organized by columns and by indices (equivalent to rows). While columns and indices can be used numerically (i.e., the first index and column are labeled \"0\", the next index and column are labeled \"1\", and so forth), it is usually desirable to give the columns or indices human-readable strings as names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a31654-aa28-4e77-90ae-a02272f6d6b4",
   "metadata": {},
   "source": [
    "##### Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a21eee-8cf7-462d-bad8-2ecb0b3aa500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As always, we start with an import statement\n",
    "import pandas as pd                          # 'pd' is the standard alias for pandas\n",
    "\n",
    "# Let's import some data to work with from a CSV file\n",
    "df = pd.read_csv(\"iris.csv\", header=None)    # 'df' is the standard variable name for a generic DataFrame\n",
    "                                             # We use \"header=None\" because there is no row dedicated to just column names in the csv\n",
    "# Let's look at the first few rows of this DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b5f7ec-e5dd-49a2-b9a4-7db13a26f679",
   "metadata": {},
   "source": [
    "##### Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbf907a-a9c4-40e0-bbbf-30afb2c99e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We had to pass the 'header=None' argument because this CSV file does not contain explicit headers. Let's rename the columns\n",
    "df = df.rename(columns={0: \"sepal_length\", 1: \"sepal_width\", 2: \"petal_length\", 3: \"petal_width\", 4: \"species\"})\n",
    "df.head()    # Notice that the data here will be converted to numeric form as appropriate rather than string form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637f6e51-f54a-4500-9c4e-b0500cdb38bd",
   "metadata": {},
   "source": [
    "##### Look at Index and Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd5c273-05c5-4f3f-8414-a94daf5682b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every DataFrame has indices and columns. The .index attribute has row names and the .columns attribute has column names.\n",
    "print(\"Row names in the DataFrame:\")\n",
    "print(df.index)\n",
    "print(\"\\nColumn names in the DataFrame:\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd26c11c-813c-4fdf-aaf6-d103ccc62c15",
   "metadata": {},
   "source": [
    "#### Selecting Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9401f087-e200-45a2-a382-5aa0fc445f16",
   "metadata": {},
   "source": [
    "We often want to look at only one or some of the columns in a DataFrame at a time. We can display or perform operations with only a few columns at a time by selecting the desired columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56daf3fb-45b0-4c74-9328-d767af02f0fa",
   "metadata": {},
   "source": [
    "##### Select from One Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363c56f5-0e04-441f-85ef-7c123c1d57ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's select only certain parts of the DataFrame. What are the values of the 'sepal_width' column?\n",
    "sw_df = df[\"sepal_width\"]        # This is the syntax for selecting an entire column\n",
    "print(\"Here is the \\\"sepal_width\\\" column of the DataFrame:\")\n",
    "print(sw_df)\n",
    "print(\"\\nNote that selecting a column from a DataFrame returns a\", type(sw_df), \"object instead of a DataFrame.\")\n",
    "print(\"A Series is a one-dimensional analog of a DataFrame. The syntax for a Series is similar, but not identical, to that for a DataFrame.\")\n",
    "\n",
    "# We can select values from this Series in a similar way as with a NumPy array\n",
    "print(\"\\nValue of \\\"sepal_width\\\" at index 2:\", sw_df[2])\n",
    "print(\"Indices 3 (inclusive) through 6 (exclusive) of \\\"sepal_width\\\":\")\n",
    "print(df[\"sepal_width\"][3:6])    # Note that it isn't necessary to create a new object to interact with a single column\n",
    "print(\"Indices 146 (inclusive) through the second-to-last index (exclusive) of \\\"sepal_width\\\":\")\n",
    "print(df[\"sepal_width\"][146:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c906472-f211-4cea-b9c0-d3b89b099270",
   "metadata": {},
   "source": [
    "##### Select from Two or More Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d12e887-7760-4744-9150-50201c9f6e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we want to analyze two columns simultaneously?\n",
    "two_col_df = df[[\"sepal_width\", \"petal_width\"]]         # This is the syntax for selecting multiple entire columns\n",
    "print(\"Here are the \\\"sepal_width\\\" and \\\"petal_width\\\" columns of the DataFrame:\")\n",
    "print(two_col_df)\n",
    "print(\"\\nNote that selecting multiple columns from a DataFrame returns a\", type(two_col_df), \"object, not a Series.\")\n",
    "\n",
    "# We can NOT select values from this DataFrame in a similar way as with a NumPy array or a Series\n",
    "try:\n",
    "    print(\"\\nValue of \\\"sepal_width\\\" and \\\"petal_width\\\" at index 2:\", two_col_df[2])\n",
    "except KeyError:\n",
    "    print(\"\\nThe input index 2 was assumed to be a column name, not an index. We have to use different syntax.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c423a89f-e94b-46c6-a086-4b5ab6bbb1ff",
   "metadata": {},
   "source": [
    "#### Selecting Slices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f5972b-947f-4865-946c-ed284ab30666",
   "metadata": {},
   "source": [
    "Selecting slices is a shortcut to simultaneously select only certain columns _and_ only certain indices. Slice selection can occur using integers with _.iloc[]_ syntax or by using index and column names with _.loc[]_ syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d80f285-a541-4790-b3dc-f5ea7937b8be",
   "metadata": {},
   "source": [
    "##### Using 'iloc' Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70701ec-cb95-4867-9ba6-ae4423970326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To select like a NumPy array, we use .iloc[] syntax, which treats indices and columns as integer positions rather than named values\n",
    "\n",
    "# Here is the syntax for selecting row numbers from a DataFrame with .iloc[]\n",
    "print(\"\\nValue of \\\"sepal_width\\\" and \\\"petal_width\\\" at index 2:\")\n",
    "print(two_col_df.iloc[2])\n",
    "print(\"Indices 3 (inclusive) through 6 (exclusive) of \\\"sepal_width\\\" and \\\"petal_width\\\":\")\n",
    "print(df[[\"sepal_width\", \"petal_width\"]].iloc[3:6])\n",
    "print(\"Indices 146 (inclusive) through the second-to-last index (exclusive) of \\\"sepal_width\\\" and \\\"petal_width\\\":\")\n",
    "print(two_col_df.iloc[146:-2])\n",
    "\n",
    "# We can select columns by integer position as well, starting from zero\n",
    "print(\"\\nRow three, column two:\", df.iloc[3,2])\n",
    "print(\"Rows one to three (exclusive), columns two to four (exclusive):\")\n",
    "print(df.iloc[1:3,2:4])\n",
    "print(\"All rows, columns from one to the second-to-last::\")\n",
    "print(df.iloc[:,1:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41308bfc-00f8-4135-a2b7-0bd42dca513f",
   "metadata": {},
   "source": [
    "##### Using 'loc' Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5b45a0-5d65-4ce2-8ef2-2d9116448900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices and columns are normally treated as named values rather than integers\n",
    "\n",
    "# Here is the syntax for selecting indices, NOT row number, from a DataFrame with .loc[]\n",
    "print(\"\\nValue of \\\"sepal_width\\\" and \\\"petal_width\\\" at index 2:\")\n",
    "print(two_col_df.loc[2])\n",
    "print(\"Indices 3 (inclusive) through 6 (ALSO INCLUSIVE) of \\\"sepal_width\\\" and \\\"petal_width\\\":\")\n",
    "print(df[[\"sepal_width\", \"petal_width\"]].loc[3:6])      # This selection is inclusive at the end because it uses index, not row number\n",
    "print(\"Indices 146 (inclusive) through the second-to-last index (exclusive) of \\\"sepal_width\\\" and \\\"petal_width\\\":\")\n",
    "print(two_col_df.loc[146:-2])                           # This syntax assumes \"-2\" is an index name, but \"-2\" isn't in the index\n",
    "\n",
    "# To select rows and columns by name, we use .loc[] syntax.\n",
    "print(\"\\nRow 2, column \\\"sepal_length\\\":\", df.loc[2,\"sepal_length\"])\n",
    "print(\"Rows 2 through 4 INCLUSIVE, columns \\\"petal_length\\\" through \\\"species\\\" INCLUSIVE:\")\n",
    "print(df.loc[2:4,\"petal_length\":\"species\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3607c81-961c-4142-8e83-9ed35798fade",
   "metadata": {},
   "source": [
    "#### Advanced DataFrame Editing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17da7d7a-4957-4ddb-956d-d20c7a8f2e12",
   "metadata": {},
   "source": [
    "In addition to changing column names, we can edit DataFrames by changing index names and adding columns manually. It's always advisable to double-check your code before changing index names since in many cases the index itself contains information about a subject of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09f88c2-1543-4a63-9510-e3f99399ee47",
   "metadata": {},
   "source": [
    "##### Change Index Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1161a936-d392-4ed0-8f5a-973070e19f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To further illustrate the difference between indices and row numbers, note that we can change index values to strings\n",
    "df.rename(index={0:\"a\", 1:\"b\", 2:\"c\"}, inplace=True)    # Using \"inplace=True\" means we edit the DataFrame directly, no \"df =\" needed\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba3ae14-8d66-45f2-851e-30eccfd6faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We reset the DataFrame here so as to not cause problems with the code later\n",
    "df = pd.read_csv(\"iris.csv\", header=None)\n",
    "df = df.rename(columns={0: \"sepal_length\", 1: \"sepal_width\", 2: \"petal_length\", 3: \"petal_width\", 4: \"species\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58da98c5-9a87-4a0f-a901-ed2451d50b5c",
   "metadata": {},
   "source": [
    "##### Add Columns Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f00b20-c394-479d-8960-4fcfbcc58ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new column is quite simple. The easiest way is to call the new column from the DataFrame\n",
    "df[\"extra_column\"] = 0    # This fills the whole columns with 0's\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189283fa-acf3-472e-a064-514278e34344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now change the values of the new column to be whatever we wish\n",
    "for i, idx in enumerate(df.index):\n",
    "    df.loc[idx,\"extra_column\"] = i**2\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecef52c7-22e2-4668-9a09-b854ce6cb29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can copy values from a different column of a DataFrame into our new column too\n",
    "df[\"extra_column\"] = df[\"sepal_width\"].copy()    # Using .copy() is VERY IMPORTANT; otherwise changes in one column will reflect in the other\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56ba6df-9527-4922-bbac-a09dc106ee8b",
   "metadata": {},
   "source": [
    "#### Null Value Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9683c97-352e-4461-a5d8-5a06bfdd3417",
   "metadata": {},
   "source": [
    "A very common challenge in data science, especially in medical informatics, is missing data. There are several ways to handle missing values, some more sophisticated than others. Sometimes our best course of action is to simply drop rows or columns with many null or missing values from consideration. Other times it's best to fill the null values with a value that is representative of the population of interest. Many more advanced techniques also exist, all of which fall beyond the scope of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88224a77-ba67-439d-b1ca-1a463475ae97",
   "metadata": {},
   "source": [
    "##### Drop Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795bb541-cd05-4efe-ad02-70e79c275729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We sometimes need to ignore parts of the data that are missing important information. Let's add missingness to our data for illustration.\n",
    "df[\"extra_column\"].iloc[np.random.choice(np.arange(len(df)), len(df)//10)] = np.nan\n",
    "df[\"extra_column\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1641140c-ae01-4f98-821e-64430fb5cf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are few enough missing values in our DataFrame, we can just drop the rows that contain missing values\n",
    "print(\"Previous number of rows:\", len(df))\n",
    "df_drop_rows = df.dropna(axis=\"index\")    # By not using \"inplace=True\", we can keep an original copy and an edited copy of the DataFrame\n",
    "print(\"New number of rows:\", len(df_drop_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed973c49-2819-439c-998d-dbdbd01cf3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that this does NOT cause the index to renumber itself\n",
    "df_drop_rows.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439cd5a1-2973-4e73-a9f7-d3b3885a30c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can drop an entire column if there are too many missing values for the column to be useful.\n",
    "df[\"extra_column\"].iloc[np.random.choice(np.arange(len(df)), len(df)//2)] = np.nan\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98e548f-88a9-4787-986e-fcc8073bf39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=\"columns\")    # Note that this will drop ALL columns that have ANY missing values.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca822880-b688-4f71-9c6f-5f6f132ab07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In many cases, it's safer to manually drop columns with high levels of missingness than it is to drop all columns with missing data\n",
    "df.drop(columns=\"species\", inplace=True)    # Using \"inplace=True\" means we don't use \"df =\" syntax\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fefe7ab-e1e1-4fa4-8672-e3a704dc1920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We reset the DataFrame here so as to not cause problems with the code later\n",
    "df = pd.read_csv(\"iris.csv\", header=None)\n",
    "df = df.rename(columns={0: \"sepal_length\", 1: \"sepal_width\", 2: \"petal_length\", 3: \"petal_width\", 4: \"species\"})\n",
    "df[\"extra_column\"] = df[\"sepal_width\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614fd70b-8a14-4fc7-97a7-91e49a60d26f",
   "metadata": {},
   "source": [
    "##### Fill Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab844677-ec68-4501-9bde-1e379c8d23cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes we want to replace missing values with an approximation of what they might have been. This is called imputation.\n",
    "df[\"extra_column\"].iloc[np.random.choice(np.arange(len(df)), len(df)//10)] = np.nan     # Adding missingness for illustration purposes\n",
    "df[\"extra_column\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822754f9-00a5-4780-a5fe-4979baaad1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple method for data imputation is to use the average of the known values in a column\n",
    "column_mean = df[\"extra_column\"].mean()\n",
    "print(\"We will replace null values with the column mean, which is\", column_mean)\n",
    "df[\"extra_column\"].fillna(df[\"extra_column\"].mean(), inplace=True)\n",
    "print(df[\"extra_column\"].values)\n",
    "print(\"Note that the imputed values have a very different level of precision than the true values.\")\n",
    "print(\"This may be valid or invalid based on your particular dataset or problem.\")\n",
    "print(\"For example, a dataset with only integer values might not behave well with this kind of data imputation. Be careful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef3628a-673d-4eca-996f-c05d76cc3c02",
   "metadata": {},
   "source": [
    "#### More Advanced DataFrame Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8d30c3-9d47-4fc1-9d0a-946b7b114f52",
   "metadata": {},
   "source": [
    "A very powerful capability of DataFrames is the ability to view or operate on only a specific subset of the data, even if that subset occupies an unknown location within a DataFrame or exists scattered across several different DataFrames. You can select indices based on whatever criteria you wish. The possibilities are endless so long as you know how to translate your ideas into code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b022356-7a34-40ab-a562-08d42f4414c1",
   "metadata": {},
   "source": [
    "##### Select Where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6358802c-c513-44d4-b8ac-615d8bff5736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can select parts of a DataFrame based on more advanced criteria\n",
    "print(\"All flowers with top decile sepal length:\")\n",
    "top_decile = np.percentile(df[\"sepal_length\"],90)\n",
    "df.where(df[\"sepal_length\"] >= top_decile).dropna()    # .where() returns the original DataFrame with non-selected rows as null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f04b6a-c464-4ef1-ac71-3d1dd35682cc",
   "metadata": {},
   "source": [
    "##### Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260f7648-f391-4bfd-a10c-347ac1bf0843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can combine DataFrames with similar data. Let's make some fake data by converting two dictionaries to DataFrames\n",
    "dict_1 = {\"Taste\":{\"Pizza Planet\":5, \"A-1 Pizza\":7, \"Just Ray's Pizza\":6}, \n",
    "          \"Store Size\":{\"Pizza Planet\":7, \"A-1 Pizza\":2, \"Just Ray's Pizza\":5}, \n",
    "          \"Manager\":{\"Pizza Planet\":\"Andy\", \"A-1 Pizza\":\"Leonardo\", \"Just Ray's Pizza\":\"Fred\"}\n",
    "         }\n",
    "dict_2 = {\"Distance\":{\"Pizza Planet\":200, \"A-1 Pizza\":100, \"Pizza Factory\":300}}\n",
    "df_1 = pd.DataFrame(dict_1)\n",
    "df_2 = pd.DataFrame(dict_2)\n",
    "print(\"DataFrame 1:\")\n",
    "print(df_1)\n",
    "print(\"\\nDataFrame 2:\")\n",
    "print(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304f8616-e4db-43fd-a09f-4cde1e57318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whichever DataFrame is on the \"left\" determines the index for the new DataFrame and the order of the columns\n",
    "df_1_on_left = df_1.join(df_2)\n",
    "df_2_on_left = df_2.join(df_1)\n",
    "print(\"Join with DataFrame 1 on the left:\")\n",
    "print(df_1_on_left)\n",
    "print(\"\\nJoin with DataFrame 2 on the left:\")\n",
    "print(df_2_on_left)\n",
    "print(\"\\nNote that index-column pairs that did not exist before the join are assigned a NaN value.\")\n",
    "print(\"We also cannot join DataFrames with duplicate column names. We can get around this by only selecting some of the columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cee75b-3a69-4e33-91bf-33cd5863c702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we do not want any NaN values resulting from indices that only occur in one DataFrame, we can use an \"inner\" join\n",
    "df_inner = df_1.join(df_2, how=\"inner\")\n",
    "print(\"Inner join:\")\n",
    "print(df_inner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5487b54-7f39-482d-9046-c1c7047a275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to preserve all indices, we can use an \"outer\" join\n",
    "df_outer = df_1.join(df_2, how=\"outer\")\n",
    "print(\"Outer join:\")\n",
    "print(df_outer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356ebde2-fac5-4842-a407-6387157c9ad8",
   "metadata": {},
   "source": [
    "##### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516e089b-0f1a-4d81-8beb-13d8436929e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging allows us to combine DataFrames based on columns rather than the index. Let's make some new data for this.\n",
    "dict_3 = {\"Sales\":{\"Andy\":9500, \"Leonardo\":6700, \"Fred\":15200}, \n",
    "          \"Height (cm)\":{\"Andy\":175, \"Leonardo\":196, \"Fred\":180}\n",
    "         }\n",
    "df_3 = pd.DataFrame(dict_3)\n",
    "print(\"DataFrame 3:\")\n",
    "print(df_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ce7778-d375-4804-b18d-cb694a249350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's merge based on manager name, which is df_3's index but a column in df_1\n",
    "df_merge = df_1.merge(df_3, left_on=\"Manager\", right_on=df_3.index)\n",
    "print(\"Merged:\")\n",
    "print(df_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251e7c42-ba04-4aaa-965b-128b08badf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the resulting DataFrame has an arbitrary, numeric index. We can use 'set_index' to set \"Manager\" as the index if we wish.\n",
    "df_merge_manager = df_merge.set_index(\"Manager\")\n",
    "print(\"Merged with \\\"Manager\\\" as index:\")\n",
    "print(df_merge_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815f2e80-f14b-4e25-93ec-306775672316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we're confident that row order was preserved we can use the index from df_1 instead\n",
    "df_merge_original_index = df_merge.rename(index={i:idx for i,idx in enumerate(df_1.index)})\n",
    "print(\"Merged with index from DataFrame 1:\")\n",
    "print(df_merge_original_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e413662-c569-4162-8ff7-4a3f090e50e8",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac796b6-7aad-4293-bf61-25a00a7c34eb",
   "metadata": {},
   "source": [
    "We as humans have a far better intuition for visual data than we do for numbers. It can be extremely useful to look at data visually before performing other kinds of data analysis to get an intuition for what kind of analyses would be appropriate. Moreover, in many instances you may be required to present your findings in a visual format, and it's important to make these presentations easy to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ef442f-258c-415c-af74-7e3b5677198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's re-import the data just in case\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"iris.csv\", header=None)\n",
    "df = df.rename(columns={0: \"sepal_length\", 1: \"sepal_width\", 2: \"petal_length\", 3: \"petal_width\", 4: \"species\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69faae50-a1a1-47f1-8fa1-049848c5e9e7",
   "metadata": {},
   "source": [
    "### Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78458f86-d808-4aa0-b1c4-9c325cf9f965",
   "metadata": {},
   "source": [
    "Another strength of Pandas is its ability to quickly produce displays from DataFrames. While Pandas may not have the same visualization versatility as other packages in Python, it is one of the easier packages to use because of its direct connection to the data you already have.  \n",
    "A useful guide for more Pandas visualization techniques: https://pandas.pydata.org/docs/user_guide/visualization.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905cc5cd-72cc-48ea-8607-38ac5203b746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to make a quick and dirty visualization of a Pandas DataFrame, we can simply use the \".plot()\" function\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba4e9a-2ee4-46cb-a02e-10d02a5f76f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# That really was not a very useful visualization at all. Let's try refining it.\n",
    "df.plot(kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997e8b42-d10d-4b39-8602-321f9a9b0c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# That's better, but the bars of the histograms all overlap each other. Let's fix that.\n",
    "df.plot(kind=\"hist\", subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db60a04d-3471-4fba-9135-a1e2f995bb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make the plot easier to read by arranging the subplots in a 2x2 grid layout and increasing the figure size\n",
    "df.plot(kind=\"hist\", subplots=True, layout=(2,2), figsize=(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbd4335-7734-4770-81c6-780386afd2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just look at one column of data\n",
    "df[\"petal_width\"].plot(kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4300f2d9-b2f8-4482-8f73-d7b354d89747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we're only interested in higher-level data such as quartiles and outliers, we can use a box plot\n",
    "df.plot(kind=\"box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08d3932-998a-4384-9886-06c6f76a1462",
   "metadata": {},
   "source": [
    "### Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6a8f7c-97d9-458a-8cb0-58b54c02ee11",
   "metadata": {},
   "source": [
    "The backend for plotting in Pandas uses the Matplotlib library, which is perhaps the most comprehensive data visualization library in all of Python. Matplotlib is extremely useful, and its basics are fairly easy to master, but there is far too much functionality available to cover adequately here. I highly recommend looking through the Matplotlib documentation and tutorials for more information.  \n",
    "https://matplotlib.org/stable/tutorials/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedb492b-88ff-4f74-8b45-69a1952af19d",
   "metadata": {},
   "source": [
    "#### Pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97936ba5-232e-4778-b1d2-894f07f8471f",
   "metadata": {},
   "source": [
    "Pyplot is the package within Matplotlib that you will likely use the most. It is the standard interface for Matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a211e40-ea7c-45ba-afd6-6985a50ef7f4",
   "metadata": {},
   "source": [
    "##### Basic Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d464e5-ae5c-4e92-aca9-22feabc73aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We always start with import statements, and the import statement for Pyplot is a little different\n",
    "from matplotlib import pyplot as plt    # plt is the standard alias for pyplot in the Matplotlib community\n",
    "# Let's plot a histogram by calling the \".hist()\" function from Pyplot on our data\n",
    "plt.hist(df[\"petal_length\"])\n",
    "# It's good form to always add a title and label your axes\n",
    "plt.title(\"Iris Dataset: Petal Length\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Petal Length (cm)\")\n",
    "# The \".show()\" function displays the graph that you have built so far\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356baeac-0aec-41f5-ab14-23c07bca6e83",
   "metadata": {},
   "source": [
    "##### Subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1b08eb-aef6-4e66-9418-2ee642c94582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use subplots to plot multiple graphs next to each other at once\n",
    "\n",
    "# Here's the graph for petal length\n",
    "plt.subplot(221)    # This tells Pyplot to put the following graph in the first spot of a 2x2 grid\n",
    "plt.hist(df[\"petal_length\"])\n",
    "# It's good form to always add a title and label your axes\n",
    "plt.title(\"Petal Length\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Petal Length (cm)\")\n",
    "\n",
    "# Here's the graph for petal width\n",
    "plt.subplot(222)    # This tells Pyplot to put the following graph in the second spot of a 2x2 grid\n",
    "plt.hist(df[\"petal_width\"])\n",
    "plt.title(\"Petal Width\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Petal Width (cm)\")\n",
    "\n",
    "# Here's the graph for sepal width\n",
    "plt.subplot(2,2,3)    # This alternate syntax tells Pyplot to put the following graph in the third spot of a 2x2 grid\n",
    "plt.hist(df[\"sepal_width\"])\n",
    "plt.title(\"Sepal Width\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Sepal Width (cm)\")\n",
    "\n",
    "# Here's the graph for sepal length\n",
    "plt.subplot(2,2,4)    # This alternate syntax tells Pyplot to put the following graph in the fourth spot of a 2x2 grid\n",
    "plt.hist(df[\"sepal_length\"])\n",
    "plt.title(\"Sepal Length\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Sepal Length (cm)\")\n",
    "\n",
    "# We can add a title above the whole figure in addition to the titles in each subplot\n",
    "plt.suptitle(\"Iris Dataset\")\n",
    "\n",
    "# Display the graphs\n",
    "plt.tight_layout()    # Using plt.tight_layout() is important to prevent axis labels and titles from overlapping each other\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681e2e12-2603-4d1a-8f01-a957f9e89e11",
   "metadata": {},
   "source": [
    "##### Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c63db5c-6d95-41ac-bdf5-b7f7c48b29b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the edges of the bars of our histograms are spaced at weird intervals. We can use the \"bins\" argument to change that.\n",
    "plt.hist(df[\"petal_length\"], bins=np.linspace(0,8,9))    # np.linspace() creates an evenly spaced array; 9 numbers between 0 and 8 (incl)\n",
    "# As always, we add a title and axis labels\n",
    "plt.title(\"Iris Dataset: Petal Length\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Petal Length (cm)\")\n",
    "# Display the new plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82af9270-5dbb-408f-a6a8-07a7f9b455cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we would like finer granularity, we can add more bins by making a longer array with np.linspace()\n",
    "plt.hist(df[\"petal_length\"], bins=np.linspace(0,8,81))\n",
    "# As always, we add a title and axis labels\n",
    "plt.title(\"Iris Dataset: Petal Length\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Petal Length (cm)\")\n",
    "# Display the new plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cff9fa9-00e8-47d5-997a-95dbd70095c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# That granularity might be too fine. Let's try that again\n",
    "plt.hist(df[\"petal_length\"], bins=np.linspace(0,8,41))\n",
    "# As always, we add a title and axis labels\n",
    "plt.title(\"Iris Dataset: Petal Length\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Petal Length (cm)\")\n",
    "# Display the new plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc31b39f-e91d-4e6a-9c60-f001386ca1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we would like to see proportions rather than absolute counts, we can use the \"density\" argument\n",
    "plt.hist(df[\"petal_length\"], bins=np.linspace(0,8,41), density=True)\n",
    "# Since this is based on proportions, it is useful to ensure that our y-axis runs from 0 to 1 instead of the auto-generated limits\n",
    "plt.ylim(0,1)\n",
    "# As always, we add a title and axis labels\n",
    "plt.title(\"Iris Dataset: Petal Length\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.xlabel(\"Petal Length (cm)\")\n",
    "# Display the new plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c06c0d5-0f83-4369-a263-462283cf06e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can change the display color with the \"color\" argument, and we can add labels for a legend with the \"label\" argument\n",
    "plt.hist(df[\"petal_length\"], bins=np.linspace(0,8,41), density=True, color=\"g\", label=\"Petal Length\")\n",
    "# Since this is based on proportions, it is useful to ensure that our y-axis runs from 0 to 1 instead of the auto-generated limits\n",
    "plt.ylim(0,1)\n",
    "# We can add a legend if we wish\n",
    "plt.legend(loc=\"upper left\")\n",
    "# As always, we add a title and axis labels\n",
    "plt.title(\"Iris Dataset: Petal Length\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.xlabel(\"Petal Length (cm)\")\n",
    "# Display the new plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2b3caf-a77a-4cd6-b5e2-e95a702f4a77",
   "metadata": {},
   "source": [
    "#### Mplot 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f6f3bd-b517-4ae1-a0b8-ad2dbdd1a67a",
   "metadata": {},
   "source": [
    "Plotting in 3D can be very useful, although it is somewhat more complicated. Instead of using basic Pyplot syntax, we must use more advanced syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc1eccd-2f65-4670-9021-cf1da88d875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first create a figure object and an axes object with a 3D projection\n",
    "fig = plt.figure(figsize=(9,9))    # We set the figure size here if we don't want to use the default size\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "# We use the axes object to add a title and labels\n",
    "ax.set_title(\"Iris Dataset\")\n",
    "ax.set_xlabel(\"Petal Length\")\n",
    "ax.set_ylabel(\"Petal Width\")\n",
    "ax.set_zlabel(\"Sepal Width\")\n",
    "\n",
    "# We can plot each entry in the iris dataset as a point in 3D space using a 3D scatterplot\n",
    "ax.scatter(df[\"petal_length\"], df[\"petal_width\"], df[\"sepal_width\"])\n",
    "\n",
    "# We can set the viewing angle (elevation and azimuth) before displaying the data\n",
    "ax.view_init(20, 25)\n",
    "\n",
    "# We display the graph as usual\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d33d566-1da1-4405-a876-a98c6a6debf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want our plot to be interactive in Jupyter Notebook, we use the following command:\n",
    "%matplotlib widget\n",
    "# Note that this will only apply to graphs generated after we run this command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ba6ad9-5483-4b84-a186-eaff09c42ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same code as above\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "# We use the axes object to add a title and labels\n",
    "ax.set_title(\"Iris Dataset\")\n",
    "ax.set_xlabel(\"Petal Length\")\n",
    "ax.set_ylabel(\"Petal Width\")\n",
    "ax.set_zlabel(\"Sepal Width\")\n",
    "\n",
    "# We can plot each entry in the iris dataset as a point in 3D space using a 3D scatterplot\n",
    "ax.scatter(df[\"petal_length\"], df[\"petal_width\"], df[\"sepal_width\"])\n",
    "\n",
    "# We can set the viewing angle (elevation and azimuth) and figure size before displaying the data\n",
    "ax.view_init(20, 25)\n",
    "\n",
    "# We display the graph as usual\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24c5d27-9aac-435e-bf03-d5cf42b193ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To revert to non-interactive graphs, we use the following command:\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57981bf-db01-4df1-a4b6-bbbd33f80353",
   "metadata": {},
   "source": [
    "### Seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd92320f-e867-4af7-9034-1389fac678de",
   "metadata": {},
   "source": [
    "Seaborn is a Python package built on top of Matplotlib. It is more focused on displaying statistical data, whereas Matplotlib is designed to be more flexible. Seaborn can more easily produce more attractive plots than Matplotlib can alone when displaying statistical information.  \n",
    "Seaborn's official tutorials: https://seaborn.pydata.org/tutorial.html  \n",
    "A perhaps more user-friendly tutorial: https://www.geeksforgeeks.org/python-seaborn-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93820bd-9b01-45b5-b087-07001a162a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As per usual, here is our import statement with the community-standard alias\n",
    "import seaborn as sns\n",
    "\n",
    "# Let's get the correlation between each of the four variables in the iris dataset\n",
    "corr = df.corr()\n",
    "print(\"Correlation matrix for each variable in the iris dataset:\")\n",
    "print(corr, end=\"\\n\\n\")\n",
    "\n",
    "# Let's visualize this data with a heatmap\n",
    "ax = sns.heatmap(corr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519949fc-5381-4798-908c-a795f4065b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It may be easier to understand the data with a different colormap; more colormaps at https://seaborn.pydata.org/tutorial/color_palettes.html\n",
    "ax = sns.heatmap(corr, cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30434dbd-8fba-4e89-aa28-05f21ef59c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can annotate the cells in the heatmap to have visual and numeric data simultaneously\n",
    "ax = sns.heatmap(corr, cmap=\"magma\", annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935eee33-70ff-479b-8a0e-0b932a9998bd",
   "metadata": {},
   "source": [
    "## Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e7e06f-2338-482b-8cd6-335c9919adb8",
   "metadata": {},
   "source": [
    "A regular expression, also known as a _regex_ or an _re_, is a powerful tool for handling textual data. In short, a regular expression acts as an advanced search query to find strings that match the query. You can practice with regular expressions at https://regex101.com.  \n",
    "This is a good example tutorial for learning regular expressions: https://towardsdatascience.com/a-very-easy-tutorial-to-learn-python-regular-expression-re-c42fbbc01ef2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400be846-d80d-4754-9bf1-172a5cce122e",
   "metadata": {},
   "source": [
    "### Basic Python Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec07100c-c8a1-4baa-a67b-ec014c3efc72",
   "metadata": {},
   "source": [
    "The basic syntax in Python for regular expressions isn't too complicated. We can save query patterns for later by compiling pattern strings into a Pattern object, then we can pass strings to be searched for the pattern into functions of the Pattern object to determine if a match exists. For most of these functions, if there is no match, None is returned (which always evaluates to False in Python), while a Match object (which always evaluates to True in Python) is returned if there is a match. Each Match object contains fairly detailed information about the match between the pattern string and the string that was searched. Other functions return lists of variable length containing strings that match the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09f5138-964e-47a6-81a7-02e48ddc293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's our import statement\n",
    "import re\n",
    "\n",
    "# We compile our query string into a Python regex object so we can use it more easily later\n",
    "cat_finder = re.compile(\"cat\")\n",
    "\n",
    "# Here are test strings to search\n",
    "test_str_1 = \"I like cats\"    # The query is in this string\n",
    "test_str_2 = \"I like dogs\"    # The query is not in this string\n",
    "\n",
    "# Calling \".search()\" on our regex object returns a match object if the any part of the string matches the regular expression, None otherwise\n",
    "search_1 = cat_finder.search(test_str_1)\n",
    "search_2 = cat_finder.search(test_str_2)\n",
    "print(\"Does \\\"{}\\\" contain {}? {}\".format(test_str_1,cat_finder,bool(search_1)))\n",
    "print(\"Does \\\"{}\\\" contain {}? {}\\n\".format(test_str_2,cat_finder,bool(search_2)))\n",
    "\n",
    "# Calling \".findall()\" on our regex object returns a list of matches\n",
    "findall_1 = cat_finder.findall(test_str_1)\n",
    "findall_2 = cat_finder.findall(test_str_2)\n",
    "print(\"How often does \\\"{}\\\" contain {}? {}\".format(test_str_1,cat_finder,len(findall_1)))\n",
    "print(\"How often does \\\"{}\\\" contain {}? {}\\n\".format(test_str_2,cat_finder,len(findall_2)))\n",
    "\n",
    "# Calling \".match()\" on our regex object returns a match object if the beginning of the string matches the regular expression, None otherwise\n",
    "match_1 = cat_finder.match(test_str_1)\n",
    "match_2 = cat_finder.match(test_str_2)\n",
    "print(\"Does \\\"{}\\\" match {}? {}\".format(test_str_1,cat_finder,bool(match_1)))\n",
    "print(\"Does \\\"{}\\\" match {}? {}\\n\".format(test_str_2,cat_finder,bool(match_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317ac3c7-c0cc-43fd-9933-40f8ed416f79",
   "metadata": {},
   "source": [
    "### Special Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3be373-694f-4043-b143-4b704d2b3d03",
   "metadata": {},
   "source": [
    "The special characters are what separates regular expressions from a simple Ctrl+F or Cmd+F search. These special characters allow us to specify allowed variants, delimit groups, and set up simple logical expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57be13c8-ad1a-4fc1-8e92-31ef9d9906cc",
   "metadata": {},
   "source": [
    "#### Character Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e6068d-64a1-4e26-812c-536fca98186c",
   "metadata": {},
   "source": [
    "Using \"\\[ \\]\" indicates that any one character, but only one character, within the square brackets (the \"set\") can count as a match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb00deaf-0d45-46df-ae07-2795d30ff9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This regex will match both \"gray\" and \"grey\", useful for if we don't know if a document uses American English or British English\n",
    "grey_gray = re.compile(\"gr[ae]y\")\n",
    "\n",
    "# Let's search each of these test strings\n",
    "test_strings = [\"gray\", \"grey\", \"groy\", \"graey\"]\n",
    "for string in test_strings:\n",
    "    if grey_gray.match(string):\n",
    "        print(string, \"matches\", str(grey_gray))\n",
    "    else:\n",
    "        print(string, \"does not match\", str(grey_gray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e8df51-703a-4f15-9687-a894ab8bd015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This regex will match a range of characters alphabetically between 'n' and 'z', the last half of the alphabet. It is case sensitive\n",
    "last_half = re.compile(\"[n-z]\")    # The '-' in this context means to look at a range of characters\n",
    "\n",
    "# Let's search each of these test strings\n",
    "test_strings = [\"a\", \"e\", \"i\", \"o\", \"u\", \"y\", \"A\", \"E\", \"I\", \"O\", \"U\", \"Y\"]\n",
    "for string in test_strings:\n",
    "    if last_half.match(string):\n",
    "        print(string, \"matches\", str(last_half))\n",
    "    else:\n",
    "        print(string, \"does not match\", str(last_half))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32606d7-04ab-468b-9ca1-971fecf90ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This regex will match a range of characters alphabetically between 'n' and 'z', the last half of the alphabet. It is case insensitive\n",
    "last_half = re.compile(\"[n-zN-Z]\")    # The '-' in this context means to look at a range of characters\n",
    "\n",
    "# Let's search each of these test strings\n",
    "test_strings = [\"a\", \"e\", \"i\", \"o\", \"u\", \"y\", \"A\", \"E\", \"I\", \"O\", \"U\", \"Y\"]\n",
    "for string in test_strings:\n",
    "    if last_half.match(string):\n",
    "        print(string, \"matches\", str(last_half))\n",
    "    else:\n",
    "        print(string, \"does not match\", str(last_half))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e112779b-96d7-4235-8bf1-73e963e0331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This regex will match any character that is NOT alphabetically between 'n' and 'z', the last half of the alphabet. It is case insensitive\n",
    "last_half = re.compile(\"[^n-zN-Z]\")    # The '^' in this context is a negator character indicating to use the complement of the set\n",
    "\n",
    "# Let's search each of these test strings\n",
    "test_strings = [\"a\", \"e\", \"i\", \"o\", \"u\", \"y\", \"A\", \"E\", \"I\", \"O\", \"U\", \"Y\"]\n",
    "for string in test_strings:\n",
    "    if last_half.match(string):\n",
    "        print(string, \"matches\", str(last_half))\n",
    "    else:\n",
    "        print(string, \"does not match\", str(last_half))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2a044-70f8-4c03-a88a-31b0ac8051c9",
   "metadata": {},
   "source": [
    "#### Built-In Sets of Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c64f3d-d26f-4e4e-80c0-a4a3c11c49e9",
   "metadata": {},
   "source": [
    "Using a \"\\\\\" followed by a certain letter will cause the regular expression to match a predefined, built-in set of characters. Here is a list of some of the more useful ones. The full list can be found at https://docs.python.org/3/library/re.html#regular-expression-syntax  \n",
    "- <b>\\d:</b> matches any digit; for ASCII characters only, this is equivalent to \\[0-9\\]  \n",
    "- <b>\\D:</b> matches anything that is _not_ a digit; for ASCII characters only, this is equivalent to \\[^0-9\\]  \n",
    "- <b>\\s:</b> matches any whitespace character\n",
    "- <b>\\S:</b> matches any character that is _not_ whitespace\n",
    "- <b>\\w:</b> matches any \"words\" character; for ASCII characters only, this is equivalent to \\[a-zA-Z0-9_\\]\n",
    "- <b>\\W:</b> matches any character that is not a \"words\" character; it is the complement of \\w\n",
    "- <b>. :</b> matches literally any character except a newline; to find an actual period only, use <b>\"\\\\.\"</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3929985c-c54b-4f55-afb2-9a906c5315ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A toy example of character matching\n",
    "toy_regex = re.compile(\"\\w\\s\\S\\w\\W\\d\\d\\D\")\n",
    "\n",
    "# Let's search each of these test strings\n",
    "test_strings = [\"I am 35!\", \"Are you 35?\", \"I  am025\", \"O my 37!\"]\n",
    "for string in test_strings:\n",
    "    if toy_regex.match(string):\n",
    "        print(\"\\\"{}\\\" matches {}\".format(string,toy_regex))\n",
    "    else:\n",
    "        print(\"\\\"{}\\\" does not match {}\".format(string,str(toy_regex)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27eed72-d0e8-4f70-977d-094a88857c86",
   "metadata": {},
   "source": [
    "#### Counting Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5250d9b0-9635-4382-8ef4-18d05e3f3353",
   "metadata": {},
   "source": [
    "Any of the following characters can be used to specify the number of times to match a certain expression.\n",
    "- <b>* :</b> matches any number of repetitions of the preceding expression, including zero times.\n",
    "- <b>+ :</b> matches any non-zero number of repetitions of the preceding expression\n",
    "- <b>? :</b> matches exactly zero or one of the preceding expression\n",
    "Using parentheses groups characters together such that the counting characters apply to the whole group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bed1352-849e-4154-b91f-8752842c23d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another toy example of character matching\n",
    "toy_regex_2 = re.compile(\"The data are( not)? reliable:\\s*\\d+ bugs? found\")\n",
    "\n",
    "# Let's search each of these test strings\n",
    "test_strings = [\"The data are reliable: 0 bugs found\\t\", \n",
    "                \"The data are not reliable: 1 bug found\", \n",
    "                \"The data are not reliable:3 bugs found\", \n",
    "                \"The data are not reliable:   18 bugs found\", \n",
    "                \"The data are not reliable:\\t18 bugs found\", \n",
    "                \"The data are not not reliable: 0 bugs found\", \n",
    "                \"The data are reliable: no bugs found\\t\", \n",
    "               ]\n",
    "for string in test_strings:\n",
    "    if toy_regex_2.match(string):\n",
    "        print(\"\\\"{}\\\" \\tDOES match the regex\".format(string))\n",
    "    else:\n",
    "        print(\"\\\"{}\\\" \\tdoes NOT match the regex\".format(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8ea327-8537-4eee-949c-dc3cd60bd478",
   "metadata": {},
   "source": [
    "#### Using 'Or'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af18d2aa-101f-41c9-bf77-f359284f01aa",
   "metadata": {},
   "source": [
    "The vertical pipe \"|\" matches if either expression on either side of it matches. These can be strung together in long chains using parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bceb48c-f119-498e-a9e3-d0177875b21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another toy example of character matching\n",
    "toy_regex_3 = re.compile(\"(Richard Nixon|George Washington|Millard Fillmore) was a president of the US\")\n",
    "\n",
    "# Let's search each of these test strings\n",
    "test_strings = [\"Richard Nixon was a president of the US\", \n",
    "                \"Nixon was a president of the US\\t\", \n",
    "                \"George Washington was a president of the US\", \n",
    "                \"Millard was a president of the US\\t\", \n",
    "                \"Millard Fillmore was a president of the US\", \n",
    "                \"Oprah Winfrey was a president of the US\"\n",
    "               ]\n",
    "for string in test_strings:\n",
    "    if toy_regex_3.match(string):\n",
    "        print(\"\\\"{}\\\" \\tDOES match the regex\".format(string))\n",
    "    else:\n",
    "        print(\"\\\"{}\\\" \\tdoes NOT match the regex\".format(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca6af96-2ae1-48ba-9f5a-9ce2bfc16510",
   "metadata": {},
   "source": [
    "### Finding Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c54454-0596-493d-ae9f-68a256974dfb",
   "metadata": {},
   "source": [
    "By delimiting groups of characters in parentheses, we can cause Python to return useful information from strings. The match object returned by \".match\" or \".search\" has a \"group\" attribute that can be used to return these groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ba5b8-9dfa-4d00-ac9a-03ebcfe0e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a DataFrame of medical information from a set of free-text strings\n",
    "medical_text = [\"The patient, Mr. Juanes, is 180 lbs. and 30 years old.\",\n",
    "                \"This patient, María, weighs 240 pounds and is 29 years old.\", \n",
    "                \"Today's patient, Ramón, weighs in at 172 pounds and turned 18 years old recently.\", \n",
    "                \"The newborn patient, Steve, only weighs 1 pound. He is 0 years old today.\"\n",
    "               ]\n",
    "\n",
    "# We initialize a dictionary to hold the information to be turned into a DataFrame\n",
    "med_info_dict = {}\n",
    "\n",
    "# We create a regex to find the name, weight, and age of each patient\n",
    "patient_info_finder = re.compile(\".+patient, ([\\w\\. ]+),\\D+(\\d+) (lbs?\\.|pounds?)\\D+(\\d+) years old\")    # Each () here is a returnable group\n",
    "# \".+\" indicates that an arbitrary, non-zero number of characters precedes the word \"patient\"\n",
    "# \"patient, \" is to ensure that the patient's name is in the immediately following section of text\n",
    "# \"([\\w\\. ]+),\" captures any non-zero number of letters, periods, and spaces preceeding a comma, which should be the patient's name\n",
    "# \"\\D+(\\d+)\" captures the first set of digits following a string of non-numeric characters\n",
    "# \" (lbs?\\.|pounds?)\" is to ensure that the string of digits corresponds to a weight in pounds; the \"s?\"s are for plurals and singulars\n",
    "# Since \" (lbs?\\.|pounds?)\" is technically a capturing group, but we don't wish to capture this group, we skip .group(3) in the code below\n",
    "# \"\\D+(\\d+)\" captures the next set of digits following a string of non-numeric characters\n",
    "# \" years old\" ensures that the previous string of digits corresponds to an age in years\n",
    "\n",
    "# We fill the dictionary with information from the regular expressions\n",
    "for text in medical_text:\n",
    "    if patient_info_finder.search(text):    # This if-statement is necessary to avoid calling \".group\" on a None object\n",
    "        name = patient_info_finder.search(text).group(1)\n",
    "        weight = patient_info_finder.search(text).group(2)\n",
    "        age = patient_info_finder.search(text).group(4)\n",
    "        med_info_dict[name] = (weight, age)\n",
    "\n",
    "# We convert the dictionary into a DataFrame\n",
    "medical_df = pd.DataFrame(columns=[\"Age (years)\", \"Weight (lbs)\"])\n",
    "for name in med_info_dict.keys():\n",
    "    medical_df.loc[name,\"Age (years)\"] = med_info_dict[name][1]\n",
    "    medical_df.loc[name,\"Weight (lbs)\"] = med_info_dict[name][0]\n",
    "medical_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc65672c-7a19-45d9-9ec6-1cd60b70692a",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2376d0a2-0090-4cab-a83c-87065e79dc71",
   "metadata": {},
   "source": [
    "https://docs.python.org/3/library/re.html  \n",
    "https://pandas.pydata.org/  \n",
    "https://numpy.org/  \n",
    "https://scipy.org/  \n",
    "https://archive.ics.uci.edu/ml/datasets/iris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79946b9",
   "metadata": {},
   "source": [
    "Made by Adam Kotter, copyright 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca10dca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
