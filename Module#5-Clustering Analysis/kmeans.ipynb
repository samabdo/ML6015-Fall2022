{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "1. Book Chapter 9.\n",
    "2. The web links and pdfs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Clustering Analysis][1]\n",
    "\n",
    "Clustering is the process of dividing the entire data into groups (also known as clusters) based on the patterns in the data.\n",
    "\n",
    "In clustering, we do not have a target to predict. We look at the data and then try to club similar observations and form different groups. Hence it is an unsupervised learning problem.\n",
    "\n",
    "## Properties of Clusters\n",
    "\n",
    "<b> Example: </b> Assume we have bank Income-Debt:\n",
    "\n",
    "<img style=\"float:center\" src=\"./images/Debt-bank.png\" alt=\"drawing\" height=\"500\" width=\"500\"/>\n",
    "\n",
    "<b> Property 1 </b>: All the data points in one should near or similar to each other. <b> Intra-cluster cohesion (compactness) </b> <br/>\n",
    "<b> Property 2 </b>: The data points from  away or different clusters should be as different as possible.  <b> Inter-cluster separation (isolation) </b>\n",
    "\n",
    "### Question: Which clustering case is better:\n",
    "\n",
    "<img style=\"float:center\" src=\"./images/Clustering.png\" alt=\"drawing\" height=\"600\" width=\"600\"/>\n",
    "\n",
    "\n",
    "[1]:https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Types of clustering algorithms][1]\n",
    "\n",
    "\n",
    "See the [link][2] for more details:\n",
    "\n",
    "1. Centroid-based clustering\n",
    "<img style=\"float:center\" src=\"./images/centroid.png\" alt=\"drawing\" height=\"200\" width=\"300\"/>\n",
    "\n",
    "2. Density-based clustering\n",
    "<img style=\"float:center\" src=\"./images/Density.png\" alt=\"drawing\" height=\"200\" width=\"300\"/>\n",
    "\n",
    "\n",
    "3. Distribution clustering\n",
    "<img style=\"float:center\" src=\"./images/Distribution.png\" alt=\"drawing\" height=\"200\" width=\"300\"/>\n",
    "\n",
    "[1]:https://developers.google.com/machine-learning/clustering/clustering-algorithms\n",
    "[2]:https://link.springer.com/content/pdf/10.1007%2Fs40745-015-0040-1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [What do we need for clustering][1]\n",
    "\n",
    "\n",
    "1. Proximity measure, either:\n",
    "  - Similarity measure $s(x_{i},x_{k})$:<b>large</b> if $x_{i},x_{k}$ are similar\n",
    "  - Dissimilarity measure $d(x_{i},x_{k})$:<b>small</b> if $x_{i},x_{k}$ are similar\n",
    "2. Criterion function to evaluate intra/inter-cluster relationships\n",
    "3. Algorithm to compute clustering: \n",
    "  - We need to optimize the criterion function.\n",
    "  - We need to evaluate hyper-paramaters.\n",
    "4. Clustering algorithm evaluation:\n",
    "  - [External evaluation (with class)][2]\n",
    "     - [Purity][3]: Purity is a measure of the extent to which clusters contain a single class: It is a measure between 0 and 1.\n",
    "       - For each cluster, count the number of data points from the most common class in said cluster. \n",
    "       - Sum over all clusters and divide by the total number of data points.\n",
    "          - Given some set of clusters $M$ and some set of classes $D$, both partinioning $N$ data points, purity is $\\frac{1}{N}\\sum_{m \\in M}{\\max_{d \\in D}|m \\cap d|}$\n",
    "        - <b> Example: </b> Assume you have three clusters and three classes in the below figure: <br/>\n",
    "          <img style=\"float:center\" src=\"./images/clusters.png\" alt=\"drawing\" height=\"200\" width=\"300\"/>\n",
    "         \n",
    "           - purity = 1/17 x (5+4+3)=0.71 <br/>   \n",
    "        \n",
    "  - [Internal evaluation (without class)][3]\n",
    "     - [Silhouette][4]:The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). \n",
    "         - Calculations: See the example in the [link][5] \n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "[1]:http://www.mit.edu/~9.54/fall14/slides/Class13.pdf\n",
    "[2]:https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html\n",
    "[3]:https://en.wikipedia.org/wiki/Cluster_analysis\n",
    "[4]: https://en.wikipedia.org/wiki/Silhouette_(clustering)\n",
    "[5]:https://stackoverflow.com/questions/23387275/how-do-you-manually-compute-for-silhouette-cohesion-and-separation-of-cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Proximity Functions][1]\n",
    "\n",
    "\n",
    "\n",
    "## Examples: [norm similarity:][2]\n",
    "\n",
    "<img style=\"float:center\" src=\"./images/Proximity functions.png\" alt=\"drawing\" height=\"300\" width=\"500\"/>\n",
    "\n",
    "\n",
    "[1]:https://www.ims.uni-stuttgart.de/institut/mitarbeiter/schulte/theses/phd/algorithm.pdf\n",
    "[2]:http://www.mit.edu/~9.54/fall14/slides/Class13.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [K-means clustering][1]\n",
    "\n",
    "Given k, the k-means (centroids), algorithm works as [follows][2]:\n",
    "\n",
    "1. Initialization: Choose k (random) data points (seeds) to be the initial centroids, cluster centers\n",
    "2. Assignment: Assign each data point to the closest centroid\n",
    "3. Recalculation: Re-compute the centroids using the current cluster memberships\n",
    "4. If a convergence criterion is not met, repeat steps 2 and 3 until thresholds:\n",
    "    - no (or minimum) re-assignments of data points to different clusters, or\n",
    "    - no (or minimum) change of centroids, or\n",
    "    - minimum decrease in the sum of squared error (SSE):\n",
    "        - $SSE = \\sum_{j=1}^{k}\\sum_{x \\in C_{j}}d(x,m_{j})^2$; where\n",
    "            - $C_{j}$ is the jth cluster.\n",
    "            - $m_{j}$ is the centroid of cluster $C_{j}$  (the mean vector of all the data points in $C_{j}$)\n",
    "            - $d(x,m_{j})$  is the (Eucledian) distance between data point $x$ and centroid $m_{j}$ \n",
    "\n",
    "\n",
    "[1]:http://benalexkeen.com/k-means-clustering-in-python/\n",
    "[2]:http://www.mit.edu/~9.54/fall14/slides/Class13.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Let us practice][1]\n",
    "\n",
    "## K=3\n",
    "\n",
    "[1]:http://benalexkeen.com/k-means-clustering-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialisation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'x': [12, 20, 28, 18, 29, 33, 24, 45, 45, 52, 51, 52, 55, 53, 55, 61, 64, 69, 72],\n",
    "    'y': [39, 36, 30, 52, 54, 46, 55, 59, 63, 70, 66, 63, 58, 23, 14, 8, 19, 7, 24]\n",
    "})\n",
    "\n",
    "\n",
    "np.random.seed(200)\n",
    "k = 3\n",
    "# centroids[i] = [x, y]\n",
    "centroids = {\n",
    "    i+1: [np.random.randint(0, 80), np.random.randint(0, 80)]\n",
    "    for i in range(k)\n",
    "}\n",
    "    \n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.scatter(df['x'], df['y'], color='k')\n",
    "colmap = {1: 'r', 2: 'g', 3: 'b'}\n",
    "for i in centroids.keys():\n",
    "    plt.scatter(*centroids[i], color=colmap[i])\n",
    "plt.xlim(0, 80)\n",
    "plt.ylim(0, 80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assignment Stage\n",
    "\n",
    "def assignment(df, centroids):\n",
    "    for i in centroids.keys():\n",
    "        # sqrt((x1 - x2)^2 - (y1 - y2)^2)\n",
    "        df['distance_from_{}'.format(i)] = (\n",
    "            np.sqrt(\n",
    "                (df['x'] - centroids[i][0]) ** 2\n",
    "                + (df['y'] - centroids[i][1]) ** 2\n",
    "            )\n",
    "        )\n",
    "    centroid_distance_cols = ['distance_from_{}'.format(i) for i in centroids.keys()]\n",
    "    df['closest'] = df.loc[:, centroid_distance_cols].idxmin(axis=1)\n",
    "    df['closest'] = df['closest'].map(lambda x: int(x.lstrip('distance_from_')))\n",
    "    df['color'] = df['closest'].map(lambda x: colmap[x])\n",
    "    return df\n",
    "\n",
    "df = assignment(df, centroids)\n",
    "print(df.head())\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.scatter(df['x'], df['y'], color=df['color'], alpha=0.5, edgecolor='k')\n",
    "for i in centroids.keys():\n",
    "    plt.scatter(*centroids[i], color=colmap[i])\n",
    "plt.xlim(0, 80)\n",
    "plt.ylim(0, 80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update Stage\n",
    "\n",
    "import copy\n",
    "\n",
    "old_centroids = copy.deepcopy(centroids)\n",
    "\n",
    "def update(k):\n",
    "    for i in centroids.keys():\n",
    "        centroids[i][0] = np.mean(df[df['closest'] == i]['x'])\n",
    "        centroids[i][1] = np.mean(df[df['closest'] == i]['y'])\n",
    "    return k\n",
    "\n",
    "centroids = update(centroids)\n",
    "    \n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = plt.axes()\n",
    "plt.scatter(df['x'], df['y'], color=df['color'], alpha=0.5, edgecolor='k')\n",
    "for i in centroids.keys():\n",
    "    plt.scatter(*centroids[i], color=colmap[i])\n",
    "plt.xlim(0, 80)\n",
    "plt.ylim(0, 80)\n",
    "for i in old_centroids.keys():\n",
    "    old_x = old_centroids[i][0]\n",
    "    old_y = old_centroids[i][1]\n",
    "    dx = (centroids[i][0] - old_centroids[i][0]) * 0.75\n",
    "    dy = (centroids[i][1] - old_centroids[i][1]) * 0.75\n",
    "    ax.arrow(old_x, old_y, dx, dy, head_width=2, head_length=3, fc=colmap[i], ec=colmap[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Repeat Assigment Stage\n",
    "\n",
    "df = assignment(df, centroids)\n",
    "\n",
    "# Plot results\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.scatter(df['x'], df['y'], color=df['color'], alpha=0.5, edgecolor='k')\n",
    "for i in centroids.keys():\n",
    "    plt.scatter(*centroids[i], color=colmap[i])\n",
    "plt.xlim(0, 80)\n",
    "plt.ylim(0, 80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that one of the reds is now green and one of the blues is now red.\n",
    "\n",
    "We now repeat until there are no changes to any of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue until all assigned categories don't change any more\n",
    "while True:\n",
    "    closest_centroids = df['closest'].copy(deep=True)\n",
    "    centroids = update(centroids)\n",
    "    df = assignment(df, centroids)\n",
    "    if closest_centroids.equals(df['closest']):\n",
    "        break\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.scatter(df['x'], df['y'], color=df['color'], alpha=0.5, edgecolor='k')\n",
    "for i in centroids.keys():\n",
    "    plt.scatter(*centroids[i], color=colmap[i])\n",
    "plt.xlim(0, 80)\n",
    "plt.ylim(0, 80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'x': [12, 20, 28, 18, 29, 33, 24, 45, 45, 52, 51, 52, 55, 53, 55, 61, 64, 69, 72],\n",
    "    'y': [39, 36, 30, 52, 54, 46, 55, 59, 63, 70, 66, 63, 58, 23, 14, 8, 19, 7, 24]\n",
    "})\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.predict(df)\n",
    "centroids = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "colors = map(lambda x: colmap[x+1], labels)\n",
    "\n",
    "plt.scatter(df['x'], df['y'], color=list(colors), alpha=0.5, edgecolor='k')\n",
    "for idx, centroid in enumerate(centroids):\n",
    "    plt.scatter(*centroid, color=colmap[idx+1])\n",
    "plt.xlim(0, 80)\n",
    "plt.ylim(0, 80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal K selection\n",
    "\n",
    "## [Rule of thumb][2]\n",
    "\n",
    "$K=\\sqrt{n/2}$; where $n$ is the number of points\n",
    "\n",
    "\n",
    "## [Elbow method (clustering)][1]\n",
    "\n",
    "1. We need to visualize the number of clusters againest:\n",
    "  - <b> Distortion: </b> It is calculated as the average of the squared distances from the cluster centers of the respective clusters (Average of $SE$). Typically, the Euclidean distance metric is used.\n",
    "  - <b> Inertia: </b> It is the sum of squared distances of samples to their closest cluster center ($SSE$)\n",
    "2. Select the one after which there will be near to platue or no steep decrement \n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Elbow_method_(clustering)\n",
    "[2]: https://stats.stackexchange.com/questions/277007/rule-of-thumb-on-the-best-k-in-k-means-clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans \n",
    "from sklearn import metrics \n",
    "from scipy.spatial.distance import cdist \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the data \n",
    "x1 = np.array([3, 1, 1, 2, 1, 6, 6, 6, 5, 6, 7, 8, 9, 8, 9, 9, 8]) \n",
    "x2 = np.array([5, 4, 5, 6, 5, 8, 6, 7, 6, 7, 1, 2, 1, 2, 3, 2, 3]) \n",
    "X = np.array(list(zip(x1, x2))).reshape(len(x1), 2) \n",
    "  \n",
    "#Visualizing the data \n",
    "plt.plot() \n",
    "plt.xlim([0, 10]) \n",
    "plt.ylim([0, 10]) \n",
    "plt.title('Dataset') \n",
    "plt.scatter(x1, x2) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = [] \n",
    "inertias = [] \n",
    "mapping1 = {} \n",
    "mapping2 = {} \n",
    "K = range(1,10) \n",
    "  \n",
    "for k in K: \n",
    "    #Building and fitting the model \n",
    "    kmeanModel = KMeans(n_clusters=k).fit(X) \n",
    "    kmeanModel.fit(X)     \n",
    "      \n",
    "    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, \n",
    "                      'euclidean'),axis=1)) / X.shape[0]) \n",
    "    inertias.append(kmeanModel.inertia_) \n",
    "  \n",
    "    mapping1[k] = sum(np.min(cdist(X, kmeanModel.cluster_centers_, \n",
    "                 'euclidean'),axis=1)) / X.shape[0] \n",
    "    mapping2[k] = kmeanModel.inertia_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,val in mapping1.items(): \n",
    "    print(str(key)+' : '+str(val)) \n",
    "\n",
    "plt.plot(K, distortions, 'bx-') \n",
    "plt.xlabel('Values of K') \n",
    "plt.ylabel('Distortion') \n",
    "plt.title('The Elbow Method using Distortion') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,val in mapping2.items(): \n",
    "    print(str(key)+' : '+str(val))\n",
    "plt.plot(K, inertias, 'bx-') \n",
    "plt.xlabel('Values of K') \n",
    "plt.ylabel('Inertia') \n",
    "plt.title('The Elbow Method using Inertia') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strengths and Weaknesses\n",
    "\n",
    "#### Strengths\n",
    "1. Efficient \n",
    "2. Easy to implement\n",
    "3. A point can change cluster after updating centroids\n",
    "\n",
    "#### Weaknesses\n",
    "1. Need to specify k in advance (may be difficult if you don’t have labels)\n",
    "2. Sensitive to outliers(noise) in the data\n",
    "3. Not good at finding clusters with odd shapes (prefers spherical data)\n",
    "4. Can get stuck at local minima (depends on where initialize centroids)\n",
    "5. sensitive to scale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
