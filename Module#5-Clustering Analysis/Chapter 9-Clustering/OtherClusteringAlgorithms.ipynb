{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae6682f3",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "1. Chapter 9-- the book.\n",
    "2. The web-links in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47e056c",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "1. Understanding the main concepts of algorithms and metrics.\n",
    "2. Understanding the algorthematic logic in details:\n",
    "    - K-means.\n",
    "    - HC.\n",
    "    - DBSAN and HDBSAN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869c8736",
   "metadata": {},
   "source": [
    "# Clustering algorithms\n",
    "\n",
    "1. Partitioning models: \n",
    "    - K-means vs. K-modiods.\n",
    "2. Hierarchical models:\n",
    "    - HC\n",
    "3. [Subspace/projected models](https://en.wikipedia.org/wiki/Clustering_high-dimensional_data#Subspace_clustering): High-dimensionality space.\n",
    "4. Mixture models: Mixure space.\n",
    "    - [Guisian Mixure models (Expectation Maximization (EM) model)](https://towardsdatascience.com/gaussian-mixture-models-d13a5e915c8e)\n",
    "5. Graph-based models: Spatial (Corrdinate-based/relationship) space.\n",
    "    - [Spectral models](https://towardsdatascience.com/spectral-clustering-aba2640c0d5b#:~:text=Spectral%20clustering%20is%20a%20technique,non%20graph%20data%20as%20well)\n",
    "6. Density models: Condense/(Noise or sparse) space\n",
    "    - [DBSCAN-Density-Based Spatial Clustering of Applications with Noise](https://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html)\n",
    "        - [OPTICS-Ordering points to identify the clustering structure](https://xzz201920.medium.com/optics-d80b41fd042a)\n",
    "    - HDBSAN\n",
    "7. Others\n",
    "\n",
    "# [Availablity In scikit-learn](https://scikit-learn.org/stable/modules/clustering.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f9d61b",
   "metadata": {},
   "source": [
    "# Metrics in Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fe983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9bb332",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a0a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(metrics.homogeneity_completeness_v_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f9e2a",
   "metadata": {},
   "source": [
    "# [Graph-based clustering](https://web.iitd.ac.in/~bspanda/graphclustering.pdf)\n",
    "\n",
    "1. Graph-Based clustering uses the proximity graph\n",
    "    - Start with the proximity matrix\n",
    "    - Consider each point as a node in a graph\n",
    "    - Each edge between two nodes has a weight which is the proximity between the two points\n",
    "    - Initially the proximity graph is fully connected\n",
    "    - MIN (single-link) and MAX (complete-link) can be viewed as starting with this graph\n",
    "2. In the simplest case, clusters are connected components in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d624c4",
   "metadata": {},
   "source": [
    "## Minumum spanning tree\n",
    "\n",
    "<img style=\"float:center\" src=\"./images/ST.png\" alt=\"drawing\" height=\"600\" width=\"600\"/>\n",
    "\n",
    "<img style=\"float:center\" src=\"./images/MST.png\" alt=\"drawing\" height=\"600\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89e1a9a",
   "metadata": {},
   "source": [
    "## K-Spanning tree clustering\n",
    "\n",
    "<img style=\"float:center\" src=\"./images/KST.png\" alt=\"drawing\" height=\"600\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a87639",
   "metadata": {},
   "source": [
    "# Density-based clustering\n",
    "\n",
    "Density-Based Clustering refers to  unsupervised learning methods that identify distinctive groups/clusters in the data, based on the idea that a cluster in a data space is a contiguous region of high point density, separated from other such clusters by contiguous regions of low point density. The data points in the separating regions of low point density are typically considered noise/outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed83d4e2",
   "metadata": {},
   "source": [
    "## Density-Based Spatial Clustering of Applications with Noise (DBSCAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23045189",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "<b> minPts: </b> The minimum number of points (a threshold) clustered together for a region to be considered dense.  \n",
    "<b> eps (ε): </b> A distance measure that will be used to locate the points in the neighborhood of any point.  \n",
    "\n",
    "\n",
    "### Concepts\n",
    "\n",
    "<b> Reachability </b> in terms of density establishes a point to be reachable from another if it lies within a particular distance (eps) from it.\n",
    "\n",
    "<b> Connectivity </b>, on the other hand, involves a transitivity based chaining-approach to determine whether points are located in a particular cluster. For example, p and q points could be connected if p->r->s->t->q, where a->b means b is in the neighborhood of a.\n",
    "\n",
    "<img style=\"float:center\" src=\"./images/DBSCAN.png\" alt=\"drawing\" height=\"600\" width=\"600\"/>\n",
    "\n",
    "\n",
    "### Parameter estimation: [Link](https://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html)\n",
    "\n",
    "\n",
    "### [Algorithm](https://www.machinecurve.com/index.php/2020/12/09/performing-dbscan-clustering-with-python-and-scikit-learn/)\n",
    "\n",
    "1. We set values for 𝜖 and minPts.\n",
    "2. We randomly select a point from the samples that has not been checked before.\n",
    "3. We retrieve the 𝜖−neighborhood for this point. If it equals or exceeds minPts, we signal it as a cluster. Otherwise, we label it as noise.\n",
    "4. We signal the 𝜖−neighborhood as being part of the cluster. This means that for each point of that neighborhood, its own 𝜖−neighborhood is added to the cluster as well, and so on, and so on. We continue until no further point can be added to the cluster. Note that the point originally labeled as noise can now also become part of this cluster (it may be part of the 𝜖−neighborhood of one of the other points), or of another cluster later, because:\n",
    "5. We now start at (2) again, unless all points have been checked and labeled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a7f300",
   "metadata": {},
   "source": [
    "### DBSCAN python implementation using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768c293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate sample data\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4,\n",
    "                            random_state=1)\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Compute DBSCAN\n",
    "db = DBSCAN(eps=0.3, min_samples=10).fit(X)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "print(\"Adjusted Rand Index: %0.3f\"\n",
    "      % metrics.adjusted_rand_score(labels_true, labels))\n",
    "print(\"Adjusted Mutual Information: %0.3f\"\n",
    "      % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, labels))\n",
    "\n",
    "# Plot result\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=14)\n",
    "\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ce82b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non-spherical data\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "X, y = make_circles(n_samples=750, factor=0.3, noise=0.1,random_state=1)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "db=DBSCAN(eps=0.3, min_samples=10)\n",
    "y_pred = db.fit_predict(X)\n",
    "labels = db.labels_\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y_pred)\n",
    "print('Number of clusters: {}'.format(len(set(y_pred[np.where(y_pred != -1)]))))\n",
    "print('Homogeneity: {}'.format(metrics.homogeneity_score(y, y_pred)))\n",
    "print('Completeness: {}'.format(metrics.completeness_score(y, y_pred)))\n",
    "print(\"V-measure: %0.7f\" % metrics.v_measure_score(y, labels))\n",
    "print(\"Adjusted Rand Index: %0.7f\"\n",
    "      % metrics.adjusted_rand_score(y, labels))\n",
    "print(\"Adjusted Mutual Information: %0.7f\"\n",
    "      % metrics.adjusted_mutual_info_score(y, labels))\n",
    "print(\"Silhouette Coefficient: %0.7f\"\n",
    "      % metrics.silhouette_score(X, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953c3430",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-means\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "km=KMeans(n_clusters=2,random_state=1)\n",
    "y_pred = km.fit_predict(X)\n",
    "labels = km.labels_\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y_pred)\n",
    "print('Number of clusters: {}'.format(len(set(y_pred[np.where(y_pred != -1)]))))\n",
    "print('Homogeneity: {}'.format(metrics.homogeneity_score(y, y_pred)))\n",
    "print('Completeness: {}'.format(metrics.completeness_score(y, y_pred)))\n",
    "print(\"V-measure: %0.7f\" % metrics.v_measure_score(y, labels))\n",
    "print(\"Adjusted Rand Index: %0.7f\"\n",
    "      % metrics.adjusted_rand_score(y, labels))\n",
    "print(\"Adjusted Mutual Information: %0.7f\"\n",
    "      % metrics.adjusted_mutual_info_score(y, labels))\n",
    "print(\"Silhouette Coefficient: %0.7f\"\n",
    "      % metrics.silhouette_score(X, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b2f5fc",
   "metadata": {},
   "source": [
    "## [OPTICS-Ordering points to identify the clustering structure](https://xzz201920.medium.com/optics-d80b41fd042a)\n",
    "\n",
    "The basic idea is similar to DBSCAN, but it addresses one of DBSCAN’s major weaknesses: the problem of detecting meaningful clusters in data of varying density.       \n",
    "\n",
    "To do so, the points of the database are (linearly) ordered such that spatially closest points become neighbors in the ordering. Additionally, a special distance is stored for each point that represents the density that must be accepted for a cluster so that both points belong to the same cluster. This is represented as a dendrogram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c642172d",
   "metadata": {},
   "source": [
    "## [HDBSCAN](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html#transform-the-space)\n",
    "\n",
    "It extends DBSCAN by converting it into a hierarchical clustering algorithm, and then using a technique to extract a flat clustering based in the stability of clusters.\n",
    "\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. Transform the space according to the density/sparsity.\n",
    "2. Build the minimum spanning tree of the distance weighted graph.\n",
    "3. Construct a cluster hierarchy of connected components.\n",
    "4. Condense the cluster hierarchy based on minimum cluster size.\n",
    "5. Extract the stable clusters from the condensed tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63ddaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.datasets as data\n",
    "%matplotlib inline\n",
    "sns.set_context('poster')\n",
    "sns.set_style('white')\n",
    "sns.set_color_codes()\n",
    "plot_kwds = {'alpha' : 0.5, 's' : 80, 'linewidths':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5324ba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "moons, _ = data.make_moons(n_samples=50, noise=0.05)\n",
    "blobs, _ = data.make_blobs(n_samples=50, centers=[(-0.75,2.25), (1.0, 2.0)], cluster_std=0.25)\n",
    "test_data = np.vstack([moons, blobs])\n",
    "plt.scatter(test_data.T[0], test_data.T[1], color='b', **plot_kwds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502396b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "help (hdbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c745cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, gen_min_span_tree=True)\n",
    "clusterer.fit(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec81153",
   "metadata": {},
   "source": [
    "#### Transform the space according to the density/sparsity.\n",
    "\n",
    "It uses DBSCAN to find the islands of higher density amid a sea of sparser noise by finding core and reachable distances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2c28ab",
   "metadata": {},
   "source": [
    "#### Build the minimum spanning tree of the distance weighted graph.\n",
    "\n",
    "Now consider a threshold value, starting high, and steadily being lowered. Drop any edges with weight above that threshold. As we drop edges we will start to disconnect the graph into connected components. Eventually we will have a hierarchy of connected components (from completely connected to completely disconnected) at varying threshold levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0310fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.minimum_spanning_tree_.plot(edge_cmap='viridis',\n",
    "                                      edge_alpha=0.6,\n",
    "                                      node_size=80,\n",
    "                                      edge_linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dae877",
   "metadata": {},
   "source": [
    "#### Construct a cluster hierarchy of connected components.\n",
    "\n",
    "Given the minimal spanning tree, the next step is to convert that into the hierarchy of connected components. This is most easily done in the reverse order: sort the edges of the tree by distance (in increasing order) and then iterate through, creating a new merged cluster for each edge.  \n",
    "\n",
    "Remeber single-linkage approach is to link pairs with the closest values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cdee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.single_linkage_tree_.plot(cmap='viridis', colorbar=True)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFQAAAAYCAYAAABk8drWAAABYWlDQ1BrQ0dDb2xvclNwYWNlRGlzcGxheVAzAAAokWNgYFJJLCjIYWFgYMjNKykKcndSiIiMUmB/yMAOhLwMYgwKicnFBY4BAT5AJQwwGhV8u8bACKIv64LMOiU1tUm1XsDXYqbw1YuvRJsw1aMArpTU4mQg/QeIU5MLikoYGBhTgGzl8pICELsDyBYpAjoKyJ4DYqdD2BtA7CQI+whYTUiQM5B9A8hWSM5IBJrB+API1klCEk9HYkPtBQFul8zigpzESoUAYwKuJQOUpFaUgGjn/ILKosz0jBIFR2AopSp45iXr6SgYGRiaMzCAwhyi+nMgOCwZxc4gxJrvMzDY7v////9uhJjXfgaGjUCdXDsRYhoWDAyC3AwMJ3YWJBYlgoWYgZgpLY2B4dNyBgbeSAYG4QtAPdHFacZGYHlGHicGBtZ7//9/VmNgYJ/MwPB3wv//vxf9//93MVDzHQaGA3kAFSFl7jXH0fsAAAA4ZVhJZk1NACoAAAAIAAGHaQAEAAAAAQAAABoAAAAAAAKgAgAEAAAAAQAAAFSgAwAEAAAAAQAAABgAAAAACaNaMgAAAs5JREFUWAntmU2oDWEYx4+PhWQhSTYWYkO+KUUWUrKTpRJTCCkrsrC2oiTJtVBYUiQLSwsf26sUJepEcW2kLC34/c6Zp15zzbn36syZce7512/er5n3feaZ533fOXNardmjLdzqh6pvd37VAzSk/0vYsRqWNcSeoTDDCP1R9Z3MrXqA2dZ/0aEZDvgKPs2R+uSBp/RT+eLdJ1tn0s1ApvzfDNpH5S8YtiitzaE62cX7lpkhUq0O1ZmV74gDfFgZY7mUOfO8t4HPvuX54E7/kWbggTkl5+rQT/Acdpeck1b79BenFT3ybdoO9GgfuiadqYPOglOkDjnu/8of/nJt8ZVJp6qfoGNHmqYH0imvM5/ARpjIr39IugFW5eWyxIiucsrHG8fRMgOaVu/m465e3P0sO/WK9VQNVNrnL7iQ+ak2TGdZbXb3MnAcw4zUOqVzUoe6DMWyVGaX52RljbO9vujQqfwR5w/coU3+Hqoz9sN7WAIhp/odOA+3QeddhLewBp7BUlBHYBfE2utMewE78/QyaQbXYAyU300d85wFlEHYsY18vEb6jVW7vuVpjEGxeXLtS6d4sewm6JRWblixnurcDNQ4ZGYS+QYTSn8J2p/OVvYRbcVx7dN2xzMfMt+xoakRehoDX4a1pJ+TvNnvSfk++cfwER5ARBbZSTIyjSy1AHTOhAVk5CrLtinteNPJdQ+b8/yJPI2H2s7LraY6NOybTvqKk1bAITiVX1B0qpGm0tfCk92qfz62udIlQ0XaKn5g7jbXf7yOCTsSMzaRX5iU0+yNvOBNGYGucyqiWGf6X9JBMNoiIiMKI8pomiTtiP5stC+5Cal9Ud+aR0MT9SU3ykjzhvaCPzAWgTafgZXwCLyxPeDmcxgugNe7Bjtlt4Ob1ms4Bm469vcOjoP1XrcW7sFV2AqOdRfs6wqsg/VgnZuWst+0vlM5OvTRA78BySqsCbsSJGoAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "id": "3bb1b528",
   "metadata": {},
   "source": [
    "#### Condense the cluster hierarchy based on minimum cluster size.\n",
    "\n",
    "Given:  \n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "We need to merge clusters such that each cluster has the size determined by the cluster size paramter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99084db",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.condensed_tree_.plot()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "e1b9e401",
   "metadata": {},
   "source": [
    "#### Extract the stable clusters from the condensed tree.\n",
    "\n",
    "For a given cluster we can then define values $\\lambda_{\\mathrm{birth}}$ and $\\lambda_{\\mathrm{death}}$ to be the lambda value when the cluster split off and became it’s own cluster, and the lambda value (if any) when the cluster split into smaller clusters respectively. \n",
    "\n",
    "In turn, for a given cluster, for each point p in that cluster we can define the value $\\lambda_p$ as the lambda value at which that point ‘fell out of the cluster’ which is a value somewhere between $\\lambda_{\\mathrm{birth}}$ and $\\lambda_{\\mathrm{death}}$ since the point either falls out of the cluster at some point in the cluster’s lifetime, or leaves the cluster when the cluster splits into two smaller clusters.Now, for each cluster compute the stability as\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "\n",
    "Declare all leaf nodes to be selected clusters. Now work up through the tree (the reverse topological sort order). If the sum of the stabilities of the child clusters is greater than the stability of the cluster, then we set the cluster stability to be the sum of the child stabilities. If, on the other hand, the cluster’s stability is greater than the sum of its children then we declare the cluster to be a selected cluster and unselect all its descendants. Once we reach the root node we call the current set of selected clusters our flat clustering and return that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f28bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.condensed_tree_.plot(select_clusters=True, selection_palette=sns.color_palette())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55745ed",
   "metadata": {},
   "source": [
    "#### Plot the resultant clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0d9d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette()\n",
    "cluster_colors = [sns.desaturate(palette[col], sat)\n",
    "                  if col >= 0 else (0.5, 0.5, 0.5) for col, sat in\n",
    "                  zip(clusterer.labels_, clusterer.probabilities_)]\n",
    "plt.scatter(test_data.T[0], test_data.T[1], c=cluster_colors, **plot_kwds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
