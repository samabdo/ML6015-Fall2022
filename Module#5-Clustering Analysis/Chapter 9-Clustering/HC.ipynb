{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resouces\n",
    "\n",
    "1. The links in this lecture\n",
    "2. [Cheat sheet for implementing 7 methods for selecting the optimal number of clusters in Python](https://towardsdatascience.com/cheat-sheet-to-implementing-7-methods-for-selecting-optimal-number-of-clusters-in-python-898241e1d6ad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [What is Hierarchical Clustering (HC)?][1]\n",
    "\n",
    "Hierarchical Clustering is an unsupervised learning algorithm that merges similar clusters of unlabeled data. Hierarchical Clustering is different from K-Means, it does not require any prior knowledge about the number of clusters K and the output is a dendrogram, a tree structure hierarchy of clusters.\n",
    "\n",
    "\n",
    "# [Types of HC][2]\n",
    "\n",
    "1. Agglomerative ‚Äî Bottom up approach. Start with many small clusters and merge them together to create bigger clusters.\n",
    "2. Divisive ‚Äî Top down approach. Start with a single cluster than break it up into smaller clusters.\n",
    "\n",
    "## [Linkage (distance) concept][3]\n",
    "\n",
    "<img style=\"float:center\" src=\"./images/Linkages.png\" alt=\"drawing\" height=\"200\" width=\"300\"/>\n",
    "\n",
    "1. Single Linkage ‚Äì the distance between the two clusters is defined as the shortest distance two points in each cluster.\n",
    "<img style=\"float:center\" src=\"./images/minLink.png\" alt=\"drawing\" height=\"100\" width=\"200\"/>\n",
    "\n",
    "\n",
    "\n",
    "2. Complete Linkage ‚Äì the distance between two clusters is defined as the longest distance between two points in each cluster.\n",
    "<img style=\"float:center\" src=\"./images/maxLink.png\" alt=\"drawing\" height=\"100\" width=\"200\"/>\n",
    "\n",
    "\n",
    "\n",
    "3. Average Linkage ‚Äì the distance between two clusters is defined as the average distance between each point in one cluster to every point in the other cluster.\n",
    "<img style=\"float:center\" src=\"./images/avgLink.png\" alt=\"drawing\" height=\"200\" width=\"200\"/>\n",
    "\n",
    "4. Ward‚Äôs Method: This approach of calculating the similarity between two clusters is exactly the same as Group Average except that Ward‚Äôs method calculates the sum of the square of the distances between two clusters.\n",
    "\n",
    "\n",
    "[1]:https://codingwithalex.com/hierarchical-clustering/\n",
    "[2]:https://towardsdatascience.com/machine-learning-algorithms-part-12-hierarchical-agglomerative-clustering-example-in-python-1e18e0075019\n",
    "[3]:https://towardsdatascience.com/understanding-the-concept-of-hierarchical-clustering-technique-c6e8243758ec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some pros and cons of Hierarchical Clustering\n",
    "## Pros\n",
    "1. No assumption of a particular number of clusters (i.e. k-means)\n",
    "2. May correspond to meaningful taxonomies\n",
    "\n",
    "## Cons\n",
    "1. Once a decision is made to combine two clusters, it can‚Äôt be undone\n",
    "2. Too slow for large data sets, O(ùëõ2 log(ùëõ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divisive Hierarchical Clustering\n",
    "\n",
    "\n",
    "<img style=\"float:center\" src=\"./images/AG1.png\" alt=\"drawing\" height=\"100\" width=\"200\"/>\n",
    "\n",
    "<img style=\"float:center\" src=\"./images/AG2.png\" alt=\"drawing\" height=\"100\" width=\"200\"/>\n",
    "\n",
    "\n",
    "# [Agglomerative Hierarchical Clustering Algorithm][1]\n",
    "\n",
    "1. Start with N separate clusters, one for each data point (N represent the number of points in your data).\n",
    "2. Compute the approximity function between clusters.\n",
    "3. Merge the two clusters that are closest to each other (based on some linkage criterion ‚Äî single linkage)\n",
    "4. Recompute distances between the clusters.\n",
    "5. Repeat steps 2 and 3 until you get one cluster of size N. The output is a dendrogram (tree hierarchy of clusters)\n",
    "\n",
    "\n",
    "1. Step 1:\n",
    "<img style=\"float:center\" src=\"./images/AHC.png\" alt=\"drawing\" height=\"100\" width=\"200\"/>\n",
    "\n",
    "2. Final HC:\n",
    "<img style=\"float:center\" src=\"./images/AHCR.png\" alt=\"drawing\" height=\"100\" width=\"200\"/>\n",
    "\n",
    "[1]:https://www.analyticsvidhya.com/blog/2019/05/beginners-guide-hierarchical-clustering/\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Example][1]\n",
    "\n",
    "1. Step 1: Data table:\n",
    "<img style=\"float:center\" src=\"./images/Matrix.png\" alt=\"drawing\" height=\"100\" width=\"200\"/>\n",
    "\n",
    "2. Step 2: Proximity Matrix:\n",
    "<img style=\"float:center\" src=\"./images/proximity matrix.png\" alt=\"drawing\" height=\"100\" width=\"200\"/>\n",
    "\n",
    "3. Step 3: Merge 1 and 2:\n",
    "<img style=\"float:center\" src=\"./images/merge_2.png\" alt=\"drawing\" height=\"100\" width=\"200\"/>\n",
    "\n",
    "4. Step 3: Table after merge 1 and 2:\n",
    "<img style=\"float:center\" src=\"./images/merge_2_max.png\" alt=\"drawing\" height=\"100\" width=\"200\"/>\n",
    "\n",
    "5. Final HC:\n",
    "<img style=\"float:center\" src=\"./images/FinalHC.png\" alt=\"drawing\" height=\"100\" width=\"200\"/>\n",
    "\n",
    "[1]:https://www.analyticsvidhya.com/blog/2019/05/beginners-guide-hierarchical-clustering/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Let us practice][1]\n",
    "\n",
    "[1]:https://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[5,3],\n",
    "    [10,15],\n",
    "    [15,12],\n",
    "    [24,10],\n",
    "    [30,30],\n",
    "    [85,70],\n",
    "    [71,80],\n",
    "    [60,78],\n",
    "    [70,55],\n",
    "    [80,91],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = range(1, 11)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.subplots_adjust(bottom=0.1)\n",
    "plt.scatter(X[:,0],X[:,1], label='True Position')\n",
    "\n",
    "for label, x, y in zip(labels, X[:, 0], X[:, 1]):\n",
    "    plt.annotate(\n",
    "        label,\n",
    "        xy=(x, y), xytext=(-3, 3),\n",
    "        textcoords='offset points', ha='right', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "linked = linkage(X, 'single')\n",
    "\n",
    "labelList = range(1, 11)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linked,\n",
    "            orientation='top',\n",
    "            labels=labelList,\n",
    "            distance_sort='descending',\n",
    "            show_leaf_counts=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(dendrogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to select K using dendrogram\n",
    "\n",
    "## Try to cut the tree before level 2,3,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SciKit-learn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[5,3],\n",
    "    [10,15],\n",
    "    [15,12],\n",
    "    [24,10],\n",
    "    [30,30],\n",
    "    [85,70],\n",
    "    [71,80],\n",
    "    [60,78],\n",
    "    [70,55],\n",
    "    [80,91],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\n",
    "cluster.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1], c=cluster.labels_, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Iris Data:\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "feat = iris.feature_names\n",
    "X = iris.data[:, :2]  # we only take the first two features. We could\n",
    "                      # avoid this ugly slicing by using a two-dim dataset\n",
    "y = iris.target\n",
    "y_name = ['Setosa', 'Versicolour', 'Virginica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "clustering = AgglomerativeClustering(linkage=\"ward\", n_clusters=3)\n",
    "clustering.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax scale the data so that it fits nicely onto the 0.0->1.0 axes of the plot.\n",
    "from sklearn import preprocessing\n",
    "X_plot = preprocessing.MinMaxScaler().fit_transform(X)\n",
    "\n",
    "colours = 'rbg'\n",
    "for i in range(X.shape[0]):\n",
    "    plt.text(X_plot[i, 0], X_plot[i, 1], str(clustering.labels_[i]),\n",
    "             color=colours[y[i]],\n",
    "             fontdict={'weight': 'bold', 'size': 9}\n",
    "        )\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "linkage_matrix = linkage(X, 'ward')\n",
    "figure = plt.figure(figsize=(7.5, 5))\n",
    "dendrogram(\n",
    "    linkage_matrix,\n",
    "    color_threshold=0,\n",
    ")\n",
    "plt.title('Hierarchical Clustering Dendrogram (Ward)')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
