{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c19f7ae4",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Reading-Guide\" data-toc-modified-id=\"Reading-Guide-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Reading Guide</a></span></li><li><span><a href=\"#Feature-engineering-for-data-preprocessing\" data-toc-modified-id=\"Feature-engineering-for-data-preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Feature engineering for data preprocessing</a></span></li><li><span><a href=\"#Adult-dataset\" data-toc-modified-id=\"Adult-dataset-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span><a href=\"https://archive.ics.uci.edu/ml/datasets/adult\" target=\"_blank\">Adult dataset</a></a></span></li><li><span><a href=\"#Imputation\" data-toc-modified-id=\"Imputation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Imputation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Working-with-missing-data-in-pandas\" data-toc-modified-id=\"Working-with-missing-data-in-pandas-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Working with missing data in pandas</a></span></li><li><span><a href=\"#Numerical-imputation\" data-toc-modified-id=\"Numerical-imputation-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Numerical imputation</a></span></li><li><span><a href=\"#Categorical-imputation\" data-toc-modified-id=\"Categorical-imputation-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Categorical imputation</a></span></li></ul></li><li><span><a href=\"#Handling_outliers\" data-toc-modified-id=\"Handling_outliers-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Handling_outliers</a></span></li><li><span><a href=\"#Log_transform\" data-toc-modified-id=\"Log_transform-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Log_transform</a></span></li><li><span><a href=\"#Handling_the_non-numeric_categories\" data-toc-modified-id=\"Handling_the_non-numeric_categories-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span><a href=\"https://towardsdatascience.com/encoding-categorical-features-21a2651a065c\" target=\"_blank\">Handling_the_non-numeric_categories</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#get_dummies:\" data-toc-modified-id=\"get_dummies:-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>get_dummies:</a></span></li><li><span><a href=\"#One-Hot-Encode-Sequence:\" data-toc-modified-id=\"One-Hot-Encode-Sequence:-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span><a href=\"https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\" target=\"_blank\">One Hot Encode Sequence:</a></a></span></li></ul></li><li><span><a href=\"#Feature_scaling\" data-toc-modified-id=\"Feature_scaling-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Feature_scaling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Scaling-MinMax-Scalor\" data-toc-modified-id=\"Scaling-MinMax-Scalor-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Scaling MinMax Scalor</a></span><ul class=\"toc-item\"><li><span><a href=\"#Norm-forms\" data-toc-modified-id=\"Norm-forms-8.1.1\"><span class=\"toc-item-num\">8.1.1&nbsp;&nbsp;</span>Norm forms</a></span><ul class=\"toc-item\"><li><span><a href=\"#L1-Norm\" data-toc-modified-id=\"L1-Norm-8.1.1.1\"><span class=\"toc-item-num\">8.1.1.1&nbsp;&nbsp;</span>L1 Norm</a></span></li></ul></li><li><span><a href=\"#L2-Norm\" data-toc-modified-id=\"L2-Norm-8.1.2\"><span class=\"toc-item-num\">8.1.2&nbsp;&nbsp;</span>L2 Norm</a></span><ul class=\"toc-item\"><li><span><a href=\"#L-infinity-Norm\" data-toc-modified-id=\"L-infinity-Norm-8.1.2.1\"><span class=\"toc-item-num\">8.1.2.1&nbsp;&nbsp;</span>L-infinity Norm</a></span></li></ul></li><li><span><a href=\"#Scaling-to-unit-length-(Normalization)\" data-toc-modified-id=\"Scaling-to-unit-length-(Normalization)-8.1.3\"><span class=\"toc-item-num\">8.1.3&nbsp;&nbsp;</span>Scaling to unit length (Normalization)</a></span></li><li><span><a href=\"#Standardization-(Z-score-Normalization)\" data-toc-modified-id=\"Standardization-(Z-score-Normalization)-8.1.4\"><span class=\"toc-item-num\">8.1.4&nbsp;&nbsp;</span>Standardization (Z-score Normalization)</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521c62c6",
   "metadata": {},
   "source": [
    "# Reading Guide\n",
    "\n",
    "<b> The reading materials are: </b>\n",
    "1. Chapter 2 in the book.\n",
    "2. The following link: [feature engineering](https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114)\n",
    "\n",
    "<b> You have two choices: either read the above materials then this notebook or you can read this notebook and just read the related topics in the above materials.</b>\n",
    "\n",
    "<b> All other web links are used as citations but if you need more information you can read them. <>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bc64fb",
   "metadata": {},
   "source": [
    "# Feature engineering for data preprocessing\n",
    "\n",
    "Using machine learning techniques requires to pre-process the data. In this step, we need to:\n",
    " - Clean the data from missing, noise, outliers data points.\n",
    " - Handle the existing features properly and/or add new features, when needed.\n",
    "  - These steps could be achieved by considering the following steps:\n",
    "    1. [Imputation](#Imputation)  \n",
    "    2. [Handling outliers](#Handling_outliers)\n",
    "    3. [Log_transform](#Log_transform).\n",
    "    4. [Handling the non-numeric categories](#Handling_the_non-numeric_categories)\n",
    "    5. [Feature_scaling](#Feature_scaling)\n",
    "\n",
    "__Not all data preprocessing steps work well in all ML algorithms. As well, you need to validate the steps using the validation subset.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b62f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline \n",
    "\n",
    "np.random.seed(0)  # For reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f54b153",
   "metadata": {},
   "source": [
    "# [Adult dataset](https://archive.ics.uci.edu/ml/datasets/adult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf2eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More about the adult data\n",
    "df= pd.read_csv(\"~/DATA/adult.csv\",na_values='?')  # Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9704da2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351ed053",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12729ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ded5d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = df._get_numeric_data().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc9f765",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065fd3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols=list(set(cols) - set(num_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b709f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an example only to fill more data with null values \n",
    "df.replace(0,np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0261f788",
   "metadata": {},
   "source": [
    "# Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d767a4e",
   "metadata": {},
   "source": [
    "## Working with missing data in pandas\n",
    "\n",
    "\n",
    "<b> As extra reading, read the [link](https://www.geeksforgeeks.org/working-with-missing-data-in-pandas/) </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935650cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check how many null values in each column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b250c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the indexes of the rows having mean value of all features in the row \n",
    "indexes=df.index[(df.isnull().mean(axis=1) < 0.1)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554347c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bbe467",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the rows having mean value of all features in the row\n",
    "df = df.loc[df.isnull().mean(axis=1) < 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87a3180",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c7092a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns with missing value rate higher than 0.70\n",
    "df = df[df.columns[df.isnull().mean() <= 0.70]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb77fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c70819b",
   "metadata": {},
   "source": [
    "## Numerical imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed438c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"~/DATA/adult.csv\",na_values='?') # Load the data\n",
    "df.replace(0,np.nan,inplace=True)  # replace the 0 with nan value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e77d0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling missing values with medians of the columns\n",
    "df = df.fillna(df.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d91895",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd387db",
   "metadata": {},
   "source": [
    "## Categorical imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a198f4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the counts of each categorical value\n",
    "for col in cat_cols:\n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca7b1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Max fill function for categorical columns\n",
    "for col in cat_cols:\n",
    "    df[col].fillna(df[col].value_counts().idxmax(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94dab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbe47cf",
   "metadata": {},
   "source": [
    "# Handling_outliers\n",
    "\n",
    "<b> There are many [outliers techniques](https://towardsdatascience.com/a-brief-overview-of-outlier-detection-techniques-1e0b2c19e561) and we can use unsupervised techniques but here we illustrate a simple technique </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581adbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat these lines to reset everything \n",
    "df= pd.read_csv(\"~/DATA/adult.csv\",na_values='?') # Load the data\n",
    "df.replace(0,np.nan,inplace=True)  # replace the 0 with nan value\n",
    "df = df.fillna(df.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f74275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[num_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3566df9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    upper_lim = df[col].quantile(.75)\n",
    "    lower_lim = df[col].quantile(.25)\n",
    "    print(col,upper_lim,lower_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1778475",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f2f637",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using quantiles to drop the outliers-- You may loose the data\n",
    "\n",
    "for col in num_cols:\n",
    "    upper_lim = df[col].quantile(.75)\n",
    "    lower_lim = df[col].quantile(.25)\n",
    "    df = df[(df[col] < upper_lim) & (df[col] > lower_lim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a859ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc2792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babdf894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat these lines to reset everything \n",
    "df= pd.read_csv(\"~/DATA/adult.csv\",na_values='?') # Load the data\n",
    "df.replace(0,np.nan,inplace=True)  # replace the 0 with nan value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81be129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using quantiles to cap the outliers with quantiles-- You may effect the data distribution\n",
    "\n",
    "for col in num_cols:\n",
    "    upper_lim = df[col].quantile(.75)\n",
    "    lower_lim = df[col].quantile(.25)\n",
    "    df.loc[(df[col] > upper_lim),col] = upper_lim\n",
    "    df.loc[(df[col] < lower_lim),col] = lower_lim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580d208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6e7e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d84daa5",
   "metadata": {},
   "source": [
    "# Log_transform\n",
    "\n",
    "<b> The aim is to handle skewed data and after transformation, the distribution becomes more approximate to normal.The log_transform also decreases the effect of the outliers, due to the normalization of magnitude differences and the model become more robust. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58296d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The example in https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114\n",
    "#Log Transform Example\n",
    "data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})\n",
    "data['log+1'] = (data['value']+1).transform(np.log)\n",
    "#Negative Values Handling\n",
    "#Note that the values are different\n",
    "data['log'] = (data['value']-data['value'].min()+1) .transform(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa84baed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fddce8",
   "metadata": {},
   "source": [
    "\n",
    "# [Handling_the_non-numeric_categories][2]\n",
    "\n",
    "\n",
    "Some ML algorithms/techniques could not handle non-numeric categories. So, we need to transform the categories into numbers.\n",
    "\n",
    "[2]:https://towardsdatascience.com/encoding-categorical-features-21a2651a065c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0d7644",
   "metadata": {},
   "source": [
    "\n",
    "## get_dummies:\n",
    "\n",
    "The idea is to encode each categorical feature with numbers (one-to-one correspondence) with its categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5d7120",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get_dummies:\n",
    "\n",
    "X=['x1','x2','x3', np.nan]\n",
    "pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f661cb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we need to handle nan as a categories\n",
    "pd.get_dummies(X,dummy_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd04633",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dealing with dataframe with categorical and numeric variables\n",
    "df = pd.DataFrame({'A': ['a', 'b', 'a'], 'B': ['b', 'a', 'c'],'C': [1, 2, 3]})\n",
    "pd.get_dummies(df, columns=['A','B'],prefix=['col1', 'col2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81f4c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d9401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to replace the dummies in df itself\n",
    "df=pd.get_dummies(df, columns=['A','B'],prefix=['col1', 'col2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934f3a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eb9ed6",
   "metadata": {},
   "source": [
    "## [One Hot Encode Sequence:][1]\n",
    "\n",
    "It works like get_dummies but it create for each category a vector of 1 for related category and zeros for the others. Some algorithms like neural network needs one hot encoding for its categorical data.\n",
    "\n",
    "\n",
    "\n",
    "[1]:https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd2e728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following code describes the difference between labelencoder and onehotencoder.\n",
    "\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# define example\n",
    "data = ['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\n",
    "values = array(data)\n",
    "print(values)\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()  #The problem of LabelEncoder is that it entails feature ordinality  \n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)\n",
    "# invert first example\n",
    "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "print(inverted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af89cf31",
   "metadata": {},
   "source": [
    "# Feature_scaling\n",
    "\n",
    "\n",
    "A numeric feature may range from a very small to a very large value. The algorithm may affect with extreme values dramatically, and such we need to represent the values to have the same appearance or similar effects:\n",
    "\n",
    "\n",
    "1. Rescaling Data\n",
    "2. Normalizing Data\n",
    "3. Standardizing Data\n",
    "\n",
    "[More information](https://machinelearningmastery.com/prepare-data-machine-learning-python-scikit-learn/)\n",
    "\n",
    "[useful_code](http://benalexkeen.com/feature-scaling-with-scikit-learn/)\n",
    "\n",
    "\n",
    "[Pima data description](https://www.kaggle.com/uciml/pima-indians-diabetes-database)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd98fa2",
   "metadata": {},
   "source": [
    "## Scaling MinMax Scalor\n",
    "\n",
    "Also known as min-max scaling or min-max normalization, is the simplest method and consists in scaling the range of features to scale the range in \\[a,b\\]\n",
    "\n",
    "To rescale $x$ into rescaled $\\hat{x}$, you need to rescale $x_{i}\\in \\hat{x}$ with the following equation:\n",
    "\n",
    "<center> $\\large \\hat{x_{i}}= a+ \\frac{(a-min(x))(b-a)}{max(x)-min(x)}$ </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54caa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rescale data (between 0 and 1)\n",
    "import pandas\n",
    "import scipy\n",
    "import numpy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "dataframe = pandas.read_csv(\"~/DATA/pima.csv\")\n",
    "array = dataframe.values\n",
    "\n",
    "X = array[:,:8]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX=scaler.fit_transform(X)\n",
    "\n",
    "numpy.set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14295694",
   "metadata": {},
   "source": [
    "### Norm forms\n",
    "\n",
    "There are different ways to measure the magnitude (size/length) of vectors, here are the most common:\n",
    "\n",
    "#### L1 Norm\n",
    "   - Manhattan Distance or Taxicab norm. L1 Norm is the sum of the magnitudes of the vectors in a space.\n",
    "   - It is commonly used in ML to deal with spareness.\n",
    "   \n",
    "   For a vector $x=(x_{1},....,x_{n})$:\n",
    "<center>$\\lVert x \\rVert_{1} = |x_{1}| + |x_{2}| +\\cdots+|x_{n}|$ </center>\n",
    "\n",
    "  \n",
    "###  L2 Norm\n",
    "   - It is the most popular norm, also known as the Euclidean norm. \n",
    "   - It is the shortest distance to go from one point to another.\n",
    "   - It is commonly used in ML to encourage small deviations from predefined values (e.g., requiring a vector of small values, and hence small deviation from zero).\n",
    "   \n",
    "   For a vector $x=(x_{1},....,x_{n})$:\n",
    "<center>$\\lVert x \\rVert_{2} = \\sqrt{x_{1}^2+x_{2}^2+\\cdots+x_{n}^2}$ </center>\n",
    "\n",
    "#### L-infinity Norm\n",
    "   - Gives the largest magnitude among each element of a vector.\n",
    "   - It is commonly used in ML to select small numbers.\n",
    "   \n",
    "   For a vector $x=(x_{1},....,x_{n})$:\n",
    "<center>$\\lVert x \\rVert_{\\infty} = max(x_{1},\\cdots,x_{n})$ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b9bec6",
   "metadata": {},
   "source": [
    "### Scaling to unit length (Normalization)\n",
    "\n",
    "Another option that is widely used in machine-learning is to scale the components of a feature vector such that the complete vector has length one.\n",
    "\n",
    "To rescale $x$ into rescaled $\\hat{x}$ you need to rescale $x_{i}\\in \\hat{x}$ with the following equation:\n",
    "\n",
    "<center> $\\large \\hat{x_{i}}=\\frac{x_{i}}{\\lVert x \\rVert_{2}}$ </center>\n",
    "\n",
    "\n",
    "given that [norm][1] as $\\lVert x \\rVert _{2}=\\sqrt{x_{1}^2+x_{2}^2+\\cdots+x_{n}^2}$ where $n$ is the size of the vector $x$\n",
    "\n",
    "\n",
    "[1]:(https://en.wikipedia.org/wiki/Norm_(mathematics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244f52ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data (length of 1)\n",
    "#Normalizing in scikit-learn refers to rescaling each observation (row) to have a length of 1 (called a unit norm in linear algebra).\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "dataframe = pandas.read_csv(\"~/DATA/pima.csv\")\n",
    "array = dataframe.values\n",
    "\n",
    "X = array[:,0:8]\n",
    "scaler = Normalizer().fit(X)\n",
    "normalizedX = scaler.transform(X)\n",
    "\n",
    "numpy.set_printoptions(precision=3)\n",
    "print(normalizedX[0:5,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff089ef8",
   "metadata": {},
   "source": [
    "### Standardization (Z-score Normalization)\n",
    "\n",
    "Another option that is widely used in ML is to scale the components of a feature vector such that the values have zero mean and unit variance (i.e., variance of 1).\n",
    "To rescale $x$ into rescaled $\\hat{x}$ to rescale $x_{i}\\in \\hat{x}$ with the following equation:\n",
    "\n",
    "<center> $\\large \\hat{x_{i}}=\\frac{x_{i}-\\bar{x}}{\\sigma}$ </center>\n",
    "\n",
    "given that $\\bar{x}=\\frac{\\sum_{i=1}^{N} x_{i}}{N}$  and $\\sigma=\\sqrt \\frac{ {\\sum_{i=1}^{N}}{(x_{i}-\\bar{x})^2}}{N-1}$where $n$ is the size of the vector $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f53cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data (0 mean, 1 stdev)\n",
    "# Standardization is a useful technique to transform attributes with a Gaussian distribution and differing means and standard deviations to a standard Gaussian distribution with a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "\"\"\"\n",
    "scaled_train =  (train - train_mean) / train_std_deviation\n",
    "scaled_test = (test - train_mean) / train_std_deviation\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import pandas\n",
    "import numpy\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "dataframe = pandas.read_csv(\"~/DATA/pima.csv\")\n",
    "array = dataframe.values\n",
    "\n",
    " \n",
    "X = array[:700,:8]\n",
    " \n",
    "scaler = StandardScaler().fit(X)\n",
    "rescaledX = scaler.transform(X)\n",
    "\n",
    "numpy.set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])\n",
    "\n",
    "std = numpy.std(rescaledX,axis=0)\n",
    "print(std)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "test=scaler.transform(array[700:,:8])\n",
    "\n",
    "print(test[0:5,:])\n",
    "\n",
    "std = numpy.std(test,axis=0)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f78fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
